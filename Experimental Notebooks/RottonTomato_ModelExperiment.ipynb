{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RottonTomato_ModelExperiment.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PXM7yVQVUo8A","colab_type":"code","outputId":"8b8eccd1-5fd6-46b2-b347-c0fadb9c9db6","executionInfo":{"status":"ok","timestamp":1566907227281,"user_tz":-330,"elapsed":7860,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}},"colab":{"base_uri":"https://localhost:8080/","height":564}},"source":["!pip install tpot"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tpot\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/4e/9ce813120662d9bd357aac6cb922f4b08b85049ed86eb47fe34a02d27f14/TPOT-0.10.2-py3-none-any.whl (75kB)\n","\r\u001b[K     |████▍                           | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 20kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 51kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 61kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.9MB/s \n","\u001b[?25hCollecting stopit>=1.1.1 (from tpot)\n","  Downloading https://files.pythonhosted.org/packages/35/58/e8bb0b0fb05baf07bbac1450c447d753da65f9701f551dca79823ce15d50/stopit-1.1.2.tar.gz\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.16.4)\n","Requirement already satisfied: pandas>=0.20.2 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.24.2)\n","Requirement already satisfied: joblib>=0.10.3 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.13.2)\n","Collecting deap>=1.0 (from tpot)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/98/3166fb5cfa47bf516e73575a1515734fe3ce05292160db403ae542626b32/deap-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (151kB)\n","\u001b[K     |████████████████████████████████| 153kB 10.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (4.28.1)\n","Collecting update-checker>=0.16 (from tpot)\n","  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n","Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.21.3)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.3.1)\n","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.2->tpot) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.2->tpot) (2.5.3)\n","Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->tpot) (2.21.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas>=0.20.2->tpot) (1.12.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n","Building wheels for collected packages: stopit\n","  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stopit: filename=stopit-1.1.2-cp36-none-any.whl size=11956 sha256=fea79381581c249b61f7ab301cbcf182be3b558d5d49926d8e4019650581c32c\n","  Stored in directory: /root/.cache/pip/wheels/3c/85/2b/2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7\n","Successfully built stopit\n","Installing collected packages: stopit, deap, update-checker, tpot\n","Successfully installed deap-1.3.0 stopit-1.1.2 tpot-0.10.2 update-checker-0.16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3FblK_CW2Uyc","colab_type":"code","outputId":"80c73e47-fca8-401b-ca47-47a2363e1fb3","executionInfo":{"status":"ok","timestamp":1566907239992,"user_tz":-330,"elapsed":7179,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["#!pip install PyDrive\n","# !pip install gensim\n","# !pip install pyldavis\n","# !python -m spacy download en\n","\n","from bs4 import BeautifulSoup\n","import requests as rq\n","import urllib.request as url\n","from bs4 import BeautifulSoup as bs\n","\n","import nltk\n","from nltk import FreqDist\n","nltk.download('stopwords') # run this one time\n","nltk.download('wordnet')\n","\n","import pandas as pd\n","pd.set_option(\"display.max_colwidth\", 200)\n","import numpy as np\n","import re\n","import spacy\n","import gzip\n","import en_core_web_sm\n","\n","import gensim\n","from gensim import corpora\n","\n","import warnings;\n","warnings.filterwarnings(\"ignore\");\n","\n","import os\n","# from pydrive.auth import GoogleAuth\n","# from pydrive.drive import GoogleDrive\n","# from oauth2client.client import GoogleCredentials\n","\n","# !pip install vecstack\n","# !pip -q install shap\n","# !pip -q install lime\n","# !pip -q install eli5\n","# !pip install tpot\n","# !pip install hyperopt\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import random as rnd\n","\n","from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","from xgboost import XGBClassifier\n","\n","#Common Model Helpers\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn import feature_selection\n","from sklearn import model_selection\n","from sklearn import metrics\n","\n","#Visualization\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pylab\n","import seaborn as sns\n","from pandas.plotting import scatter_matrix\n","\n","# Importing Models\n","from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","\n","# Importing other tools\n","from sklearn import model_selection\n","from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n","from sklearn.metrics import accuracy_score, recall_score, precision_recall_curve\n","from sklearn.model_selection import StratifiedKFold, cross_validate\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.calibration import CalibratedClassifierCV\n","\n","#library\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import random as rnd\n","\n","from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","from xgboost import XGBClassifier\n","\n","#Common Model Helpers\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn import feature_selection\n","from sklearn import model_selection\n","from sklearn import metrics\n","\n","#Visualization\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pylab\n","import seaborn as sns\n","from pandas.plotting import scatter_matrix\n","\n","# Importing Models\n","from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","\n","# Importing other tools\n","from sklearn import model_selection\n","from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n","from sklearn.metrics import accuracy_score, recall_score, precision_recall_curve\n","from sklearn.model_selection import StratifiedKFold, cross_validate\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.calibration import CalibratedClassifierCV\n","from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","#from tpot import TPOTClassifier\n","from sklearn.model_selection import KFold, ShuffleSplit, StratifiedKFold\n","import warnings\n","from sklearn.preprocessing import MinMaxScaler\n","#!pip install plotly.express\n","from imblearn.over_sampling import SMOTE\n","import itertools\n","import multiprocessing as mp\n","pool = mp.Pool(processes=mp.cpu_count())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_ZOPzzGu2Yaq","colab_type":"code","colab":{}},"source":["data = pd.read_csv(\"/content/LionKingData.csv\")\n","test = pd.read_csv(\"/content/test-1566619745327.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DidmTXPjOXSx","colab_type":"text"},"source":["#Preprocess"]},{"cell_type":"code","metadata":{"id":"Dq3zpfvkOZHY","colab_type":"code","outputId":"f7cfd77d-5627-4025-ba7d-d06e551741e0","executionInfo":{"status":"ok","timestamp":1566907257486,"user_tz":-330,"elapsed":4000,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["# dropping Unnamed: 0 , index , createDate , displayImageUrl , displayName , hasSpoilers , isVerified , rating , timeFromCreation,updateDate , user\n","\n","data['sentiment'] = [0 if x>3 else 1 for x in data['score']]\n","df1 = pd.concat([data,test])\n","df1.drop (['Unnamed: 0', 'index' ,'createDate','displayImageUrl' ,'displayName','hasSpoilers','isVerified','rating','timeFromCreation','updateDate','user' ], axis =1 , inplace = True)\n","df1.drop (['hasProfanity', 'isSuperReviewer'] , axis = 1 , inplace = True)\n","\n","cleanup_re = re.compile('[^A-Za-z0-9]+') #Regular Expresssion to store only alphabets (capital and small) and digits.\n","\n","\n","def cleanup(sentence):\n","    sentence = str(sentence)\n","    sentence = re.sub(r'\\s+',' ',sentence) #\\s is for white spaces\n","   # sentence = re.sub('[\\d]','',sentence)  #\\d is for digits\n","    sentence = sentence.lower() # converting into lower case\n","    sentence = cleanup_re.sub(' ', sentence).strip()\n","    #sentence = \" \".join(nltk.word_tokenize(sentence))\n","    return sentence\n","\n","\n","# creating a dictionoary of decontracted words that occur in common typing languages.\n","def decontracted(phrase):\n","    # specific\n","    phrase = re.sub(r\"won't\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","    phrase = re.sub(r\"don\\'t\", \"do not\" , phrase)\n","    phrase = re.sub(r\"isn\\'t\", \"is not\", phrase)\n","    phrase = re.sub(r\"he\\'s\", \"he is\", phrase)\n","    phrase = re.sub(r\"we\\'re\", \"we are\", phrase)\n","    phrase = re.sub(r\"I\\'ll\", \"I will\", phrase)\n","    phrase = re.sub(r\"you\\'re\", \"you are\", phrase)\n","    phrase = re.sub(r\"they\\'re\", \"they are\", phrase)\n","    phrase = re.sub(r\"we\\'ll\", \"we will\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n","    phrase = re.sub(r\"hasn\\'t\", \"has not\", phrase)\n","    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n","    phrase = re.sub(r\"it\\'s\", \"it is\", phrase)\n","    phrase = re.sub(r\"aren\\'t\", \"are not\", phrase)\n","    phrase = re.sub(r\"wasn\\'t\", \"was not\", phrase)\n","    phrase = re.sub(r\"weren\\'t\", \"were not\", phrase)\n","    phrase = re.sub(r\"haven\\'t\", \"have not\", phrase)\n","    phrase = re.sub(r\"hasn\\'t\", \"has not\", phrase)\n","    phrase = re.sub(r\"hadn\\'t\", \"had not\", phrase)\n","    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n","    phrase = re.sub(r\"wouldn\\'t\", \"would not\", phrase)\n","    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n","    phrase = re.sub(r\"doesn\\'t\", \"does not\", phrase)\n","    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","    phrase = re.sub(r\"shouldn\\'t\", \"should not\", phrase)\n","    phrase = re.sub(r\"mightn\\'t\", \"might not\", phrase)\n","    phrase = re.sub(r\"mustn\\'t\", \"must not\", phrase)\n","    \n","\n","    # general\n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","    return phrase\n","\n","\n","df1[\"Clean_review\"] = df1[\"review\"].apply(cleanup) # applying cleanup function on data\n","\n","df1[\"Clean_review\"] = df1[\"review\"].apply(decontracted) # applying decontracted function on web scrapped data\n","\n","df1['Clean'] = df1['Clean_review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","\n","df1['Clean'] = df1['Clean'].str.replace('[^\\w\\s\\!\\.\\?\\,\\'\\:\\;\\..\\-\\_\\()\\[]\\{}\\!!]','') # regular expression tto remove punctuation\n","\n","from textblob import Word # from textblob importing word\n","df1['Clean_lemmed'] = df1['Clean'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) # Lemmatizing the owrds using lemmatize()\n","df1['Clean_lemmed'].head().tolist() # printing the lemmatized words\n","df1.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ReviewID</th>\n","      <th>review</th>\n","      <th>score</th>\n","      <th>sentiment</th>\n","      <th>Clean_review</th>\n","      <th>Clean</th>\n","      <th>Clean_lemmed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>it okay, but like the original one.</td>\n","      <td>3.5</td>\n","      <td>0.0</td>\n","      <td>it okay, but like the original one.</td>\n","      <td>it okay, but like the original one.</td>\n","      <td>it okay, but like the original one.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>loved the whole movie</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>loved the whole movie</td>\n","      <td>loved the whole movie</td>\n","      <td>loved the whole movie</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>I genuinely don’t know where the poor critic reviews came in. I lost interest in seeing the film due to their poor reviews but eventually got around to seeing it and truly thought they did a magni...</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>I genuinely don’t know where the poor critic reviews came in. I lost interest in seeing the film due to their poor reviews but eventually got around to seeing it and truly thought they did a magni...</td>\n","      <td>i genuinely don’t know where the poor critic reviews came in. i lost interest in seeing the film due to their poor reviews but eventually got around to seeing it and truly thought they did a magni...</td>\n","      <td>i genuinely don’t know where the poor critic review came in. i lost interest in seeing the film due to their poor review but eventually got around to seeing it and truly thought they did a magnifi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>Awesome, amazing, great movie!</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>Awesome, amazing, great movie!</td>\n","      <td>awesome, amazing, great movie!</td>\n","      <td>awesome, amazing, great movie!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>Five stars.  This was not only a treat for my husband and I but also our 6 year old grandson.  So many positives.  I kept on having to tell myself these were not real animals.  The only thing that...</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>Five stars.  This was not only a treat for my husband and I but also our 6 year old grandson.  So many positives.  I kept on having to tell myself these were not real animals.  The only thing that...</td>\n","      <td>five stars. this was not only a treat for my husband and i but also our 6 year old grandson. so many positives. i kept on having to tell myself these were not real animals. the only thing that cou...</td>\n","      <td>five stars. this wa not only a treat for my husband and i but also our 6 year old grandson. so many positives. i kept on having to tell myself these were not real animals. the only thing that coul...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ReviewID  ...                                                                                                                                                                                             Clean_lemmed\n","0       NaN  ...                                                                                                                                                                      it okay, but like the original one.\n","1       NaN  ...                                                                                                                                                                                    loved the whole movie\n","2       NaN  ...  i genuinely don’t know where the poor critic review came in. i lost interest in seeing the film due to their poor review but eventually got around to seeing it and truly thought they did a magnifi...\n","3       NaN  ...                                                                                                                                                                           awesome, amazing, great movie!\n","4       NaN  ...  five stars. this wa not only a treat for my husband and i but also our 6 year old grandson. so many positives. i kept on having to tell myself these were not real animals. the only thing that coul...\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"y85Z09z5Qtgq","colab_type":"text"},"source":["#Model "]},{"cell_type":"code","metadata":{"id":"ZxhewM4DRuPs","colab_type":"code","colab":{}},"source":["# Train Data \n","train_data_index_last = 3240\n","# tv_tfidf_features[0:3239]\n","\n","# test Data\n","test_data_index_first = 3241\n","# tv_tfidf_features[3240:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0gUkq9EtQtGV","colab_type":"code","colab":{}},"source":["# TFIDF \n","\n","corpus = df1['Clean_lemmed'].tolist()\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","\n","tv = TfidfVectorizer(min_df=0.2, max_df=0.9, ngram_range=(1,3),\n","                     sublinear_tf=True,norm='l2',smooth_idf=True)\n","tfidf_features = tv.fit_transform(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1L4q875Rs_g","colab_type":"code","colab":{}},"source":["# CountVector \n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","corpus = df1['Clean_lemmed'].tolist()\n","\n","count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n","                            ngram_range=(1, 3), max_df=0.9, min_df=0.2)\n","\n","count_train = count_vec.fit(corpus)\n","cv_features = count_vec.transform(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9T0rrf-xQ8t7","colab_type":"code","colab":{}},"source":["X = tfidf_features[:train_data_index_last]\n","y = df1['sentiment'][:train_data_index_last]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGvZ3NSiTHSD","colab_type":"code","colab":{}},"source":["from imblearn.over_sampling import SMOTE\n","smt = SMOTE(random_state=777, k_neighbors=1)\n","X_SMOTE, y_SMOTE = smt.fit_sample(X, y)\n","# pd.DataFrame(X_SMOTE.todense(), columns=tv.get_feature_names())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwdW7ijZZrlw","colab_type":"code","colab":{}},"source":["\n","\n","basic_params = {\n","  'seed':123,\n","  'folds': 3,\n","  'test_size': 0.3,\n","  'train_size': 0.7,\n","  'n_jobs':-1,\n","  'verbose':3,\n","  'scoring':'f1',   \n","  'Split_type': 'ShuffleSplit',\n","  'warning': warnings.filterwarnings('ignore'),\n","  'normalize': True\n","      #StratifiedKFold() - StratifiedKFold\n","      #KFold() - KFold\n","  \n","}\n","\n","base_model = {\n","    'AdaBoostClassifier': AdaBoostClassifier(),\n","    'BaggingClassifier':BaggingClassifier(),\n","    'ExtraTreesClassifier':ExtraTreesClassifier(),\n","    'GradientBoostingClassifier':GradientBoostingClassifier(),\n","    'RandomForestClassifier':RandomForestClassifier(),\n","    'GaussianProcessClassifier':gaussian_process.GaussianProcessClassifier(),\n","    'LogisticRegressionCV':linear_model.LogisticRegressionCV(),\n","    'PassiveAggressiveClassifier':linear_model.PassiveAggressiveClassifier(),\n","    'RidgeClassifierCV':linear_model.RidgeClassifierCV(),\n","    'SGDClassifier':linear_model.SGDClassifier(),\n","    'Perceptron':linear_model.Perceptron(),\n","    'BernoulliNB':naive_bayes.BernoulliNB(),\n","    'GaussianNB':naive_bayes.GaussianNB(),\n","    'KNeighborsClassifier':KNeighborsClassifier(),\n","    'SVC':svm.SVC(probability=True),\n","    'NuSVC':svm.NuSVC(probability=True),\n","    'LinearSVC':svm.LinearSVC(),\n","    'DecisionTreeClassifier':DecisionTreeClassifier(),\n","    'LinearDiscriminantAnalysis':discriminant_analysis.LinearDiscriminantAnalysis(),\n","    'QuadraticDiscriminantAnalysis':discriminant_analysis.QuadraticDiscriminantAnalysis(),\n","    'XGBClassifier':XGBClassifier(),\n","    'LGBMClassifier':LGBMClassifier()    \n","\n","}\n","grid_model = {\n","    \n","    'LogisticRegression':LogisticRegression(),\n","    'DecisionTreeClassifier': DecisionTreeClassifier(),\n","    'RandomForestClassifier': RandomForestClassifier(),\n","    'KNeighborsClassifier': KNeighborsClassifier(),\n","    'SVC': SVC(),\n","    'ExtraTreesClassifier': ExtraTreesClassifier(),\n","    'AdaBoostClassifier': AdaBoostClassifier(),\n","    'GradientBoostingClassifier': GradientBoostingClassifier()\n","    \n","}\n","\n","grid_param = {\n","    'LogisticRegression': { \"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"] }, \n","    'DecisionTreeClassifier': {'criterion' : ['gini', 'entropy'], 'splitter' : ['random', 'best'], 'max_depth':[100,500,1000], 'min_samples_leaf':[100,200,500]},\n","    'RandomForestClassifier': { 'n_estimators': [400,600,900,1500,2000,5000] },\n","    'ExtraTreesClassifier': { 'n_estimators': [400,600,900,1500,2000,5000] },\n","    'KNeighborsClassifier':{ 'n_neighbors': [400,600,900], 'algorithm' : ['auto']},\n","    'AdaBoostClassifier':  { 'n_estimators': [400,600,900,1500,2000,5000] },\n","    'GradientBoostingClassifier': { 'n_estimators': [400,600,900,1500,2000,5000], 'learning_rate': [0.8, 1.0] },\n","    'SVC': [\n","        {'kernel': ['linear'], 'C': [1, 10]},\n","        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n","    ]\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFsUuRUWY4J-","colab_type":"code","colab":{}},"source":["class BaseModellingHelper:\n","\n","    def __init__(self, std_param, base_model):\n","        # if not set(grid_models.keys()).issubset(set(grid_params.keys()))  \\\n","        #       or not set(popt_models.keys()).issubset(set(popt_params.keys())) \\\n","        #       or not set(automl_model.keys()).issubset(set(automl_params.keys())) \\\n","        #       or not set(dl_model.keys()).issubset(set(dl_params.keys())):\n","\n","        #     missing_params_grid = list(set(grid_models.keys()) - set(grid_params.keys()))\n","        #     missing_params_popt = list(set(popt_models.keys()) - set(popt_params.keys()))\n","        #     missing_params_automl = list(set(automl_model.keys()) - set(automl_params.keys()))\n","        #     missing_params_dl = list(set(dl_model.keys()) - set(dl_params.keys()))\n","\n","        #     raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params_grid, missing_params_popt,missing_params_automl,missing_params_dl)\n","        \n","        self.std_param = std_param\n","        if self.std_param['Split_type'] == 'ShuffleSplit':\n","          self.cross_val = model_selection.ShuffleSplit(n_splits = self.std_param['folds'], test_size = self.std_param['test_size'], train_size = self.std_param['train_size'], random_state = self.std_param['seed'] )\n","        \n","        self.base_model = base_model\n","        self.base_model_output = {}\n","        self.feature_importance_df_sorted = pd.DataFrame()\n","        self.important_col =[]\n","        \n","        self.scoring = { 'accuracy' : make_scorer(metrics.accuracy_score), \n","                  'precision' : make_scorer(metrics.precision_score),\n","                  'recall' : make_scorer(metrics.recall_score), \n","                  'f1_score' : make_scorer(metrics.f1_score),\n","                  'average_precision': make_scorer(metrics.average_precision_score),\n","                  'balanced_accuracy': make_scorer(metrics.balanced_accuracy_score),\n","                  'hamming_loss':make_scorer(metrics.hamming_loss),\n","                  'jaccard_score': make_scorer(metrics.jaccard_score),\n","                  'log_loss': make_scorer(metrics.log_loss),\n","                  'roc_auc_score':make_scorer(metrics.roc_auc_score),\n","                  'zero_one_loss':make_scorer(metrics.zero_one_loss,normalize=False)\n","                  }\n","                    \n","\n","        self.scores_list = []\n","        # self.grid_searches = {}\n","        # self.best_params = {}\n","        self.feature_importance = {}\n","        self.FeatureImportanceAlgo = ['DecisionTreeClassifier','RandomForestClassifier','ExtraTreesClassifier','GradientBoostingClassifier','AdaBoostClassifier']\n","        # self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.std_param['test_size'])\n","    \n","    def getScoreDictionary(self, base_model_output,modelname, basemodel_scores, score_type):\n","      self.base_model_output[modelname]['Time'] = basemodel_scores['fit_time'].mean()\n","      self.base_model_output[modelname]['%s_accuracy' %score_type ] =  basemodel_scores['%s_accuracy' %score_type].mean()\n","      self.base_model_output[modelname]['%s_precision' %score_type ] =  basemodel_scores['%s_precision' %score_type].mean()\n","      self.base_model_output[modelname]['%s_recall' %score_type ] =  basemodel_scores['%s_recall' %score_type].mean()\n","      self.base_model_output[modelname]['%s_f1_score' %score_type ] =  basemodel_scores['%s_f1_score' %score_type].mean()\n","      self.base_model_output[modelname]['%s_average_precision' %score_type ] =  basemodel_scores['%s_average_precision' %score_type].mean()\n","      self.base_model_output[modelname]['%s_balanced_accuracy' %score_type ] =  basemodel_scores['%s_balanced_accuracy' %score_type].mean()\n","      self.base_model_output[modelname]['%s_hamming_loss' %score_type ] =  basemodel_scores['%s_hamming_loss' %score_type].mean()\n","      self.base_model_output[modelname]['%s_jaccard_score' %score_type ] =  basemodel_scores['%s_jaccard_score' %score_type].mean()\n","      self.base_model_output[modelname]['%s_log_loss' %score_type ] =  basemodel_scores['%s_log_loss' %score_type].mean()\n","      self.base_model_output[modelname]['%s_roc_auc_score' %score_type ] =  basemodel_scores['%s_roc_auc_score' %score_type].mean()\n","      self.base_model_output[modelname]['%s_zero_one_loss' %score_type ] =  basemodel_scores['%s_zero_one_loss' %score_type].mean()\n","\n","      return None\n","\n","    def ModelLoop(self,X, y, score_type=None):\n","      for key, eachModel in self.base_model.items():\n","          basemodel_scores = model_selection.cross_validate(eachModel, X,y, cv  = self.cross_val,return_train_score=True,scoring=self.scoring, pre_dispatch=\"2*n_jobs\")\n","          modelname = eachModel.__class__.__name__\n","          self.base_model_output[modelname] = {}\n","          self.getScoreDictionary(self.base_model_output,modelname, basemodel_scores, 'train')\n","          self.getScoreDictionary(self.base_model_output,modelname, basemodel_scores, 'test')\n","          self.scores_list.append(basemodel_scores)\n","          if eachModel.__class__.__name__ in self.FeatureImportanceAlgo:\n","            eachModel.fit(X,y)\n","            self.feature_importance[eachModel.__class__.__name__]= eachModel.feature_importances_\n","\n","    def runBaseLineModel(self, X, y, score_type=None, auto_feature_eng = None , top_feature = None ):\n","      if top_feature:\n","        print (\"Building model with only %s important feature\" % top_feature)\n","        #Initial Model Loop to extract top feature\n","        self.ModelLoop(X, y, score_type)\n","        imp_df = self.getFeatureImportance(self.getFeatureImportanceDF(X, self.feature_importance))\n","        important_col = list(imp_df[:top_feature].index)\n","        self.important_col = important_col\n","        X = X[important_col]\n","        self.ModelLoop(X, y,score_type)\n","      else:\n","        print (\"Building model without any important feature\")\n","        self.ModelLoop(X, y,score_type)\n","        \n","    def getFeatureImportanceDF(self, X, feature_importance_dict, important_col=None):\n","      if important_col:\n","        feature_names = important_col\n","        feat_imp_df = pd.DataFrame.from_dict(feature_importance_dict)\n","        feat_imp_df.index = feature_names\n","        return feat_imp_df\n","      else:\n","        feature_names = X.columns\n","        feat_imp_df = pd.DataFrame.from_dict(feature_importance_dict)\n","        feat_imp_df.index = feature_names\n","        return feat_imp_df\n","\n","    def getFeatureImportance(self,feat_imp_df):\n","      mms = MinMaxScaler()\n","      # scaling to MinMax Scale \n","      scaled_fi = pd.DataFrame(data=mms.fit_transform(feat_imp_df),columns=feat_imp_df.columns,index=feat_imp_df.index)\n","      # Adding all values of importance to get single socre\n","      scaled_fi['SumofImp'] = scaled_fi.sum(axis=1)\n","      # print(scaled_fi.head())\n","      ordered_ranking = scaled_fi.sort_values('SumofImp', ascending=False)\n","      return ordered_ranking\n","\n","\n","    def getFeatureImportanceGraph(self,ordered_feature_importance_df):\n","      self.feature_importance_df_sorted.append(ordered_feature_importance_df)\n","      fig, ax = plt.subplots(figsize=(10,7), dpi=80)\n","      sns.barplot(data=ordered_feature_importance_df, y=ordered_feature_importance_df.index, x='SumofImp', palette='magma')\n","      ax.spines['right'].set_visible(False)\n","      ax.spines['top'].set_visible(False)\n","      ax.spines['bottom'].set_visible(False)\n","      ax.xaxis.set_visible(False)\n","      ax.grid(False)\n","      ax.set_title('Aggregated Feature Importances for Models');\n","      return ax\n","\n","    def getModelDataframe(self, base_model_output, sort_column, asscending=False,difference_by=None, score_filter=None):\n","      score_table =  pd.DataFrame.from_dict(base_model_output).T\n","      if score_filter:\n","        score_columns = [['train_'+ eachScore,'test_'+eachScore] for eachScore in score_filter]\n","        score_columns_flat = list(itertools.chain(*score_columns))\n","        score_columns_flat.append(\"Time\")\n","        score_table = score_table[score_columns_flat]\n","      score_table['Difference_%s_unit'%difference_by] = abs(score_table['train_%s' %difference_by] - score_table['test_%s' %difference_by])*100\n","      score_table_ordered = score_table.sort_values(sort_column, ascending=asscending)\n","      return score_table_ordered\n","    \n","    def getModelValidationGraph(self, ModelDataFrame, x_col= None, Difference_bins=5,difference_col=None,size=None):\n","      ModelDataFrame['MLName'] = ModelDataFrame.index\n","      ModelDataFrame['Difference_Bin'] = pd.cut(ModelDataFrame[difference_col],Difference_bins)\n","      ax = plt.figure(figsize=(18,8))\n","      # sns.scatterplot(x=x_col, y=\"MLName\",data=ModelDataFrame,size='Time', hue='Difference_Bin',sizes=(20, 600), hue_norm=(0, 20))\n","      fig = px.scatter(ModelDataFrame, x=x_col, y=\"MLName\", color=\"Difference_Bin\",size=size)\n","      # fig.show()\n","      # ax.grid(False)\n","      # ax.set_title('Model Validation & Overfitting');\n","      return fig\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TefqyTdwY_V4","colab_type":"code","colab":{}},"source":["ModelObject = BaseModellingHelper(basic_params,base_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ev2pE4h-ZBx6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"5fd25b73-afc3-4551-ba2b-a5a3c815ac85","executionInfo":{"status":"ok","timestamp":1566921395147,"user_tz":-330,"elapsed":413584,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["%time ModelObject.runBaseLineModel(X_SMOTE.toarray(), y_SMOTE,score_type='test')"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Building model without any important feature\n","CPU times: user 7min 56s, sys: 1min 3s, total: 9min\n","Wall time: 6min 52s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iinQIooyZGOF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":751},"outputId":"2c44e7eb-1cb0-495c-d451-109f03e4bb9a","executionInfo":{"status":"ok","timestamp":1566921539064,"user_tz":-330,"elapsed":675,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["ModelObject.getModelDataframe(ModelObject.base_model_output,sort_column='test_f1_score',asscending=False,difference_by='recall',score_filter=['f1_score','precision','recall','zero_one_loss'])"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_f1_score</th>\n","      <th>test_f1_score</th>\n","      <th>train_precision</th>\n","      <th>test_precision</th>\n","      <th>train_recall</th>\n","      <th>test_recall</th>\n","      <th>train_zero_one_loss</th>\n","      <th>test_zero_one_loss</th>\n","      <th>Time</th>\n","      <th>Difference_recall_unit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ExtraTreesClassifier</th>\n","      <td>0.898249</td>\n","      <td>0.762490</td>\n","      <td>0.953454</td>\n","      <td>0.795689</td>\n","      <td>0.849208</td>\n","      <td>0.732174</td>\n","      <td>318.7</td>\n","      <td>321.2</td>\n","      <td>0.028895</td>\n","      <td>11.703340</td>\n","    </tr>\n","    <tr>\n","      <th>LGBMClassifier</th>\n","      <td>0.875168</td>\n","      <td>0.740927</td>\n","      <td>0.922770</td>\n","      <td>0.749748</td>\n","      <td>0.832332</td>\n","      <td>0.732483</td>\n","      <td>393.4</td>\n","      <td>360.7</td>\n","      <td>0.232612</td>\n","      <td>9.984930</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestClassifier</th>\n","      <td>0.892548</td>\n","      <td>0.734223</td>\n","      <td>0.939251</td>\n","      <td>0.760999</td>\n","      <td>0.850389</td>\n","      <td>0.709434</td>\n","      <td>339.2</td>\n","      <td>361.6</td>\n","      <td>0.044937</td>\n","      <td>14.095578</td>\n","    </tr>\n","    <tr>\n","      <th>BaggingClassifier</th>\n","      <td>0.891342</td>\n","      <td>0.732201</td>\n","      <td>0.937571</td>\n","      <td>0.761015</td>\n","      <td>0.849533</td>\n","      <td>0.705662</td>\n","      <td>343.2</td>\n","      <td>363.4</td>\n","      <td>0.137409</td>\n","      <td>14.387116</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsClassifier</th>\n","      <td>0.784007</td>\n","      <td>0.721151</td>\n","      <td>0.747132</td>\n","      <td>0.676305</td>\n","      <td>0.825963</td>\n","      <td>0.773969</td>\n","      <td>754.4</td>\n","      <td>421.5</td>\n","      <td>0.007848</td>\n","      <td>5.199349</td>\n","    </tr>\n","    <tr>\n","      <th>DecisionTreeClassifier</th>\n","      <td>0.898249</td>\n","      <td>0.709514</td>\n","      <td>0.953454</td>\n","      <td>0.741412</td>\n","      <td>0.849208</td>\n","      <td>0.680474</td>\n","      <td>318.7</td>\n","      <td>392.3</td>\n","      <td>0.023951</td>\n","      <td>16.873316</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingClassifier</th>\n","      <td>0.747938</td>\n","      <td>0.692915</td>\n","      <td>0.766300</td>\n","      <td>0.708070</td>\n","      <td>0.730650</td>\n","      <td>0.678777</td>\n","      <td>815.9</td>\n","      <td>423.6</td>\n","      <td>0.306928</td>\n","      <td>5.187306</td>\n","    </tr>\n","    <tr>\n","      <th>XGBClassifier</th>\n","      <td>0.734267</td>\n","      <td>0.684706</td>\n","      <td>0.757040</td>\n","      <td>0.706711</td>\n","      <td>0.712955</td>\n","      <td>0.664180</td>\n","      <td>855.1</td>\n","      <td>430.8</td>\n","      <td>0.218714</td>\n","      <td>4.877446</td>\n","    </tr>\n","    <tr>\n","      <th>SGDClassifier</th>\n","      <td>0.674163</td>\n","      <td>0.669320</td>\n","      <td>0.643300</td>\n","      <td>0.637790</td>\n","      <td>0.716285</td>\n","      <td>0.712967</td>\n","      <td>1141.6</td>\n","      <td>493.2</td>\n","      <td>0.017603</td>\n","      <td>0.331792</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostClassifier</th>\n","      <td>0.687206</td>\n","      <td>0.667420</td>\n","      <td>0.702196</td>\n","      <td>0.678519</td>\n","      <td>0.672964</td>\n","      <td>0.656951</td>\n","      <td>1015.2</td>\n","      <td>461.0</td>\n","      <td>0.201678</td>\n","      <td>1.601359</td>\n","    </tr>\n","    <tr>\n","      <th>GaussianProcessClassifier</th>\n","      <td>0.680836</td>\n","      <td>0.665415</td>\n","      <td>0.698671</td>\n","      <td>0.680659</td>\n","      <td>0.664056</td>\n","      <td>0.651401</td>\n","      <td>1031.5</td>\n","      <td>461.2</td>\n","      <td>12.399733</td>\n","      <td>1.265534</td>\n","    </tr>\n","    <tr>\n","      <th>LogisticRegressionCV</th>\n","      <td>0.664387</td>\n","      <td>0.663964</td>\n","      <td>0.664115</td>\n","      <td>0.659761</td>\n","      <td>0.666002</td>\n","      <td>0.669842</td>\n","      <td>1113.2</td>\n","      <td>477.0</td>\n","      <td>0.250909</td>\n","      <td>0.384080</td>\n","    </tr>\n","    <tr>\n","      <th>GaussianNB</th>\n","      <td>0.657503</td>\n","      <td>0.663613</td>\n","      <td>0.652011</td>\n","      <td>0.653719</td>\n","      <td>0.663173</td>\n","      <td>0.674036</td>\n","      <td>1144.8</td>\n","      <td>481.3</td>\n","      <td>0.002104</td>\n","      <td>1.086280</td>\n","    </tr>\n","    <tr>\n","      <th>RidgeClassifierCV</th>\n","      <td>0.659886</td>\n","      <td>0.661653</td>\n","      <td>0.669331</td>\n","      <td>0.667630</td>\n","      <td>0.650968</td>\n","      <td>0.656290</td>\n","      <td>1111.5</td>\n","      <td>472.7</td>\n","      <td>0.006779</td>\n","      <td>0.532199</td>\n","    </tr>\n","    <tr>\n","      <th>LinearSVC</th>\n","      <td>0.659990</td>\n","      <td>0.661509</td>\n","      <td>0.669475</td>\n","      <td>0.667199</td>\n","      <td>0.651030</td>\n","      <td>0.656427</td>\n","      <td>1111.1</td>\n","      <td>473.1</td>\n","      <td>0.021843</td>\n","      <td>0.539766</td>\n","    </tr>\n","    <tr>\n","      <th>LinearDiscriminantAnalysis</th>\n","      <td>0.660011</td>\n","      <td>0.661306</td>\n","      <td>0.669528</td>\n","      <td>0.667086</td>\n","      <td>0.651030</td>\n","      <td>0.656119</td>\n","      <td>1111.0</td>\n","      <td>473.3</td>\n","      <td>0.013266</td>\n","      <td>0.508873</td>\n","    </tr>\n","    <tr>\n","      <th>QuadraticDiscriminantAnalysis</th>\n","      <td>0.660227</td>\n","      <td>0.654212</td>\n","      <td>0.689303</td>\n","      <td>0.682275</td>\n","      <td>0.633991</td>\n","      <td>0.629059</td>\n","      <td>1080.9</td>\n","      <td>467.9</td>\n","      <td>0.003748</td>\n","      <td>0.493178</td>\n","    </tr>\n","    <tr>\n","      <th>SVC</th>\n","      <td>0.649294</td>\n","      <td>0.651522</td>\n","      <td>0.683855</td>\n","      <td>0.682913</td>\n","      <td>0.619212</td>\n","      <td>0.624374</td>\n","      <td>1106.9</td>\n","      <td>469.9</td>\n","      <td>2.915702</td>\n","      <td>0.516212</td>\n","    </tr>\n","    <tr>\n","      <th>NuSVC</th>\n","      <td>0.664523</td>\n","      <td>0.621764</td>\n","      <td>0.662077</td>\n","      <td>0.613624</td>\n","      <td>0.668693</td>\n","      <td>0.631798</td>\n","      <td>1115.3</td>\n","      <td>539.1</td>\n","      <td>2.609458</td>\n","      <td>3.689429</td>\n","    </tr>\n","    <tr>\n","      <th>BernoulliNB</th>\n","      <td>0.612975</td>\n","      <td>0.617080</td>\n","      <td>0.648795</td>\n","      <td>0.651887</td>\n","      <td>0.580933</td>\n","      <td>0.586279</td>\n","      <td>1215.7</td>\n","      <td>512.3</td>\n","      <td>0.002683</td>\n","      <td>0.534600</td>\n","    </tr>\n","    <tr>\n","      <th>PassiveAggressiveClassifier</th>\n","      <td>0.521403</td>\n","      <td>0.519623</td>\n","      <td>0.661896</td>\n","      <td>0.654352</td>\n","      <td>0.535649</td>\n","      <td>0.534221</td>\n","      <td>1325.0</td>\n","      <td>564.9</td>\n","      <td>0.008046</td>\n","      <td>0.142839</td>\n","    </tr>\n","    <tr>\n","      <th>Perceptron</th>\n","      <td>0.488668</td>\n","      <td>0.489293</td>\n","      <td>0.540079</td>\n","      <td>0.540362</td>\n","      <td>0.535613</td>\n","      <td>0.539079</td>\n","      <td>1445.4</td>\n","      <td>619.0</td>\n","      <td>0.005392</td>\n","      <td>0.346673</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               train_f1_score  ...  Difference_recall_unit\n","ExtraTreesClassifier                 0.898249  ...               11.703340\n","LGBMClassifier                       0.875168  ...                9.984930\n","RandomForestClassifier               0.892548  ...               14.095578\n","BaggingClassifier                    0.891342  ...               14.387116\n","KNeighborsClassifier                 0.784007  ...                5.199349\n","DecisionTreeClassifier               0.898249  ...               16.873316\n","GradientBoostingClassifier           0.747938  ...                5.187306\n","XGBClassifier                        0.734267  ...                4.877446\n","SGDClassifier                        0.674163  ...                0.331792\n","AdaBoostClassifier                   0.687206  ...                1.601359\n","GaussianProcessClassifier            0.680836  ...                1.265534\n","LogisticRegressionCV                 0.664387  ...                0.384080\n","GaussianNB                           0.657503  ...                1.086280\n","RidgeClassifierCV                    0.659886  ...                0.532199\n","LinearSVC                            0.659990  ...                0.539766\n","LinearDiscriminantAnalysis           0.660011  ...                0.508873\n","QuadraticDiscriminantAnalysis        0.660227  ...                0.493178\n","SVC                                  0.649294  ...                0.516212\n","NuSVC                                0.664523  ...                3.689429\n","BernoulliNB                          0.612975  ...                0.534600\n","PassiveAggressiveClassifier          0.521403  ...                0.142839\n","Perceptron                           0.488668  ...                0.346673\n","\n","[22 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"k6p5cP7TTW5O","colab_type":"code","colab":{}},"source":["class GridModellingHelper:\n","\n","    def __init__(self, std_param, grid_model,grid_param):\n","        if not set(grid_model.keys()).issubset(set(grid_param.keys())): \n","        #       or not set(popt_models.keys()).issubset(set(popt_params.keys())) \\\n","        #       or not set(automl_model.keys()).issubset(set(automl_params.keys())) \\\n","        #       or not set(dl_model.keys()).issubset(set(dl_params.keys())):\n","\n","        #     missing_params_grid = list(set(grid_models.keys()) - set(grid_params.keys()))\n","        #     missing_params_popt = list(set(popt_models.keys()) - set(popt_params.keys()))\n","        #     missing_params_automl = list(set(automl_model.keys()) - set(automl_params.keys()))\n","        #     missing_params_dl = list(set(dl_model.keys()) - set(dl_params.keys()))\n","\n","            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params_grid)\n","        \n","        self.std_param = std_param\n","        if self.std_param['Split_type'] == 'ShuffleSplit':\n","          self.cross_val = model_selection.ShuffleSplit(n_splits = self.std_param['folds'], test_size = self.std_param['test_size'], train_size = self.std_param['train_size'], random_state = self.std_param['seed'] )\n","        \n","        # self.base_model = base_model\n","        self.base_model_output = {}\n","        self.grid_model = grid_model\n","        self.grid_param = grid_param\n","        self.basemodel_scores  = {}\n","        self.feature_importance_df_sorted = pd.DataFrame()\n","        self.important_col =[]\n","        \n","        self.scoring = { 'accuracy' : make_scorer(metrics.accuracy_score), \n","                  'precision' : make_scorer(metrics.precision_score),\n","                  'recall' : make_scorer(metrics.recall_score), \n","                  'f1score' : make_scorer(metrics.f1_score),\n","                  'averageprecision': make_scorer(metrics.average_precision_score),\n","                  'balancedaccuracy': make_scorer(metrics.balanced_accuracy_score),\n","                  'hammingloss':make_scorer(metrics.hamming_loss),\n","                  'jaccardscore': make_scorer(metrics.jaccard_score),\n","                  'logloss': make_scorer(metrics.log_loss),\n","                  'rocaucscore':make_scorer(metrics.roc_auc_score),\n","                  'zerooneloss':make_scorer(metrics.zero_one_loss,normalize=False)\n","                  }\n","                    \n","        self.best_param_output = {}\n","        self.best_estimator_output = {}\n","        self.best_score_output = {}\n","        self.scores_list = []\n","        # self.grid_searches = {}\n","        # self.best_params = {}\n","        self.feature_importance = {}\n","        self.FeatureImportanceAlgo = ['DecisionTreeClassifier','RandomForestClassifier','ExtraTreesClassifier','GradientBoostingClassifier','AdaBoostClassifier']\n","        # self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.std_param['test_size'])\n","    \n","    def getScoreDictionary(self, base_model_output,modelname, basemodel_scores, score_type,agg_type):\n","      self.base_model_output[modelname]['Time'] = basemodel_scores['mean_fit_time'].mean()\n","      self.base_model_output[modelname]['%s_accuracy_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_accuracy' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_precision_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_precision' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_recall_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_recall' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_f1score_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_f1score' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_avgprecision_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_averageprecision' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_balancedaccuracy_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_balancedaccuracy' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_hammingloss_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_hammingloss' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_jaccardscore_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_jaccardscore' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_logloss_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_logloss' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_rocaucscore_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_rocaucscore' %(agg_type,score_type)].mean()\n","      self.base_model_output[modelname]['%s_zerooneloss_%s' %(score_type,agg_type) ] =  basemodel_scores['%s_%s_zerooneloss' %(agg_type,score_type)].mean()\n","\n","      return None\n","\n","    def ModelLoop(self,X, y):\n","      for key, eachModel in self.grid_model.items():\n","          basemodel_scores = model_selection.GridSearchCV(eachModel, self.grid_param[eachModel.__class__.__name__], verbose=1, \\\n","                                  cv  = self.cross_val,return_train_score=True, scoring=self.scoring, pre_dispatch=\"2*n_jobs\", n_jobs=-1,refit='accuracy')\n","          basemodel_scores.fit(X,y)\n","          self.basemodel_scores[key] = basemodel_scores.cv_results_\n","          self.best_estimator_output[key] = basemodel_scores.best_estimator_\n","          self.best_param_output[key] = basemodel_scores.best_params_\n","          self.best_score_output[key] = basemodel_scores.best_score_\n","          modelname = eachModel.__class__.__name__\n","          self.base_model_output[modelname] = {}\n","          self.getScoreDictionary(self.base_model_output,modelname, basemodel_scores.cv_results_, 'train',agg_type='std')\n","          self.getScoreDictionary(self.base_model_output,modelname, basemodel_scores.cv_results_, 'test',agg_type='std')\n","          self.getScoreDictionary(self.base_model_output,modelname, basemodel_scores.cv_results_, 'train',agg_type='mean')\n","          self.getScoreDictionary(self.base_model_output,modelname, basemodel_scores.cv_results_, 'test',agg_type='mean')\n","          self.scores_list.append(basemodel_scores)\n","          if eachModel.__class__.__name__ in self.FeatureImportanceAlgo:\n","            eachModel.fit(X,y)\n","            self.feature_importance[eachModel.__class__.__name__]= eachModel.feature_importances_\n","\n","    def runBaseLineModel(self, X, y, auto_feature_eng = None , top_feature = None):\n","      if top_feature:\n","        print (\"Building model with only %s important feature\" % top_feature)\n","        #Initial Model Loop to extract top feature\n","        self.ModelLoop(X, y)\n","        imp_df = self.getFeatureImportance(self.getFeatureImportanceDF(X, self.feature_importance))\n","        important_col = list(imp_df[:top_feature].index)\n","        self.important_col = important_col\n","        X = X[important_col]\n","        self.ModelLoop(X, y)\n","      else:\n","        print (\"Building model without any important feature\")\n","        self.ModelLoop(X, y)\n","        \n","\n","    def getFeatureImportanceDF(self, X, feature_importance_dict, important_col=None):\n","      if important_col:\n","        feature_names = important_col\n","        feat_imp_df = pd.DataFrame.from_dict(feature_importance_dict)\n","        feat_imp_df.index = feature_names\n","        return feat_imp_df\n","      else:\n","        feature_names = X.columns\n","        feat_imp_df = pd.DataFrame.from_dict(feature_importance_dict)\n","        feat_imp_df.index = feature_names\n","        return feat_imp_df\n","\n","    def getFeatureImportance(self,feat_imp_df):\n","      mms = MinMaxScaler()\n","      # scaling to MinMax Scale \n","      scaled_fi = pd.DataFrame(data=mms.fit_transform(feat_imp_df),columns=feat_imp_df.columns,index=feat_imp_df.index)\n","      # Adding all values of importance to get single socre\n","      scaled_fi['SumofImp'] = scaled_fi.sum(axis=1)\n","      # print(scaled_fi.head())\n","      ordered_ranking = scaled_fi.sort_values('SumofImp', ascending=False)\n","      return ordered_ranking\n","\n","\n","    def getFeatureImportanceGraph(self,ordered_feature_importance_df):\n","      self.feature_importance_df_sorted.append(ordered_feature_importance_df)\n","      fig, ax = plt.subplots(figsize=(10,7), dpi=80)\n","      sns.barplot(data=ordered_feature_importance_df, y=ordered_feature_importance_df.index, x='SumofImp', palette='magma')\n","      ax.spines['right'].set_visible(False)\n","      ax.spines['top'].set_visible(False)\n","      ax.spines['bottom'].set_visible(False)\n","      ax.xaxis.set_visible(False)\n","      ax.grid(False)\n","      ax.set_title('Aggregated Feature Importances for Models');\n","      return ax\n","\n","    def getModelDataframe(self, base_model_output, sort_column, asscending=False,difference_by=None, score_filter=None, agg = None):\n","      score_table =  pd.DataFrame.from_dict(base_model_output).T\n","      if score_filter:\n","        score_columns = [['train_'+ eachScore+'_'+agg,'test_'+eachScore+'_'+agg] for eachScore in score_filter]\n","        score_columns_flat = list(itertools.chain(*score_columns))\n","        score_columns_flat.append(\"Time\")\n","        score_table = score_table[score_columns_flat]\n","      score_table['Difference_%s_unit'%difference_by] = abs(score_table['train_%s_%s' %(difference_by,agg)] - score_table['test_%s_%s' %(difference_by,agg)])*100\n","      score_table_ordered = score_table.sort_values(sort_column, ascending=asscending)\n","      return score_table_ordered\n","    \n","    def getModelValidationGraph(self, ModelDataFrame, x_col= None, Difference_bins=5,difference_col=None,size=None):\n","      ModelDataFrame['MLName'] = ModelDataFrame.index\n","      ModelDataFrame['Difference_Bin'] = pd.cut(ModelDataFrame[difference_col],Difference_bins)\n","      ax = plt.figure(figsize=(18,8))\n","      # sns.scatterplot(x=x_col, y=\"MLName\",data=ModelDataFrame,size='Time', hue='Difference_Bin',sizes=(20, 600), hue_norm=(0, 20))\n","      fig = px.scatter(ModelDataFrame, x=x_col, y=\"MLName\", color=\"Difference_Bin\",size=size)\n","      # fig.show()\n","      # ax.grid(False)\n","      # ax.set_title('Model Validation & Overfitting');\n","      return fig\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wS0aJYZ8WmwP","colab_type":"code","colab":{}},"source":["ModelObjectGrid = GridModellingHelper(basic_params,grid_model,grid_param)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Np2wy3wOWnfR","colab_type":"code","outputId":"fe511f16-c4eb-428c-e2a4-76cb2bdb4e34","executionInfo":{"status":"ok","timestamp":1566920053143,"user_tz":-330,"elapsed":1095161,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["%time ModelObjectGrid.runBaseLineModel(X_SMOTE,y_SMOTE)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Building model without any important feature\n","Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    6.1s\n","[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   13.2s finished\n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    3.5s\n","[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   14.8s\n","[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   27.5s finished\n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 25.1min\n","[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 55.6min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  6.8min finished\n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  7.8min\n","[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 10.8min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 14.2min\n","[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 31.8min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 10.0min\n","[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 22.6min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  4.7min\n","[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 21.4min finished\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 3min 30s, sys: 4.19 s, total: 3min 34s\n","Wall time: 2h 33min 8s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xc8eiP78uDLH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"8981a3fb-55bb-4bff-f45c-24c59bf08fa5","executionInfo":{"status":"ok","timestamp":1566920432335,"user_tz":-330,"elapsed":1369,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["# 'LogisticRegression':LogisticRegression(),\n","#     'DecisionTreeClassifier': DecisionTreeClassifier(),\n","#     'RandomForestClassifier': RandomForestClassifier(),\n","#     'KNeighborsClassifier': KNeighborsClassifier(),\n","#     'SVC': SVC(),\n","#     'ExtraTreesClassifier': ExtraTreesClassifier(),\n","#     'AdaBoostClassifier': AdaBoostClassifier(),\n","#     'GradientBoostingClassifier': GradientBoostingClassifier()\n","# ModelObjectGrid.basemodel_scores['ExtraTreesClassifier'].keys()\n","ModelObjectGrid.best_estimator_output['ExtraTreesClassifier']"],"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n","                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                     min_impurity_decrease=0.0, min_impurity_split=None,\n","                     min_samples_leaf=1, min_samples_split=2,\n","                     min_weight_fraction_leaf=0.0, n_estimators=1500,\n","                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n","                     warm_start=False)"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"code","metadata":{"id":"eqWi5KrkvMez","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"594e55bc-9c20-48f8-c808-3cbf06110a52","executionInfo":{"status":"ok","timestamp":1566920502266,"user_tz":-330,"elapsed":1077,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["ModelObjectGrid.basemodel_scores['ExtraTreesClassifier']['mean_test_f1score'].mean()"],"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.776471338400877"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"qjLTckVVvXSx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"08d03779-c8d4-40a9-bc93-a97df61a5a1e","executionInfo":{"status":"ok","timestamp":1566920566312,"user_tz":-330,"elapsed":707,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["ModelObjectGrid.basemodel_scores['GradientBoostingClassifier']['mean_test_zerooneloss'].mean()"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["391.04999999999995"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"L3ydu0UtvIGb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":765},"outputId":"56f3c0bf-c4c1-4350-ba69-47cdb55516db","executionInfo":{"status":"ok","timestamp":1566920444934,"user_tz":-330,"elapsed":1338,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["ModelObjectGrid.best_estimator_output"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AdaBoostClassifier': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n","                    n_estimators=1500, random_state=None),\n"," 'DecisionTreeClassifier': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n","                        max_features=None, max_leaf_nodes=None,\n","                        min_impurity_decrease=0.0, min_impurity_split=None,\n","                        min_samples_leaf=100, min_samples_split=2,\n","                        min_weight_fraction_leaf=0.0, presort=False,\n","                        random_state=None, splitter='best'),\n"," 'ExtraTreesClassifier': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n","                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                      min_impurity_decrease=0.0, min_impurity_split=None,\n","                      min_samples_leaf=1, min_samples_split=2,\n","                      min_weight_fraction_leaf=0.0, n_estimators=1500,\n","                      n_jobs=None, oob_score=False, random_state=None, verbose=0,\n","                      warm_start=False),\n"," 'GradientBoostingClassifier': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n","                            learning_rate=0.8, loss='deviance', max_depth=3,\n","                            max_features=None, max_leaf_nodes=None,\n","                            min_impurity_decrease=0.0, min_impurity_split=None,\n","                            min_samples_leaf=1, min_samples_split=2,\n","                            min_weight_fraction_leaf=0.0, n_estimators=2000,\n","                            n_iter_no_change=None, presort='auto',\n","                            random_state=None, subsample=1.0, tol=0.0001,\n","                            validation_fraction=0.1, verbose=0,\n","                            warm_start=False),\n"," 'KNeighborsClassifier': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                      metric_params=None, n_jobs=None, n_neighbors=400, p=2,\n","                      weights='uniform'),\n"," 'LogisticRegression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                    multi_class='warn', n_jobs=None, penalty='l2',\n","                    random_state=None, solver='warn', tol=0.0001, verbose=0,\n","                    warm_start=False),\n"," 'RandomForestClassifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                        min_impurity_decrease=0.0, min_impurity_split=None,\n","                        min_samples_leaf=1, min_samples_split=2,\n","                        min_weight_fraction_leaf=0.0, n_estimators=5000,\n","                        n_jobs=None, oob_score=False, random_state=None,\n","                        verbose=0, warm_start=False),\n"," 'SVC': SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n","     decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n","     kernel='linear', max_iter=-1, probability=False, random_state=None,\n","     shrinking=True, tol=0.001, verbose=False)}"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"Uz5P02YPdJE3","colab_type":"code","outputId":"24a1746d-9744-4307-cd69-2bc7dc2aa051","executionInfo":{"status":"error","timestamp":1566920150663,"user_tz":-330,"elapsed":1415,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}},"colab":{"base_uri":"https://localhost:8080/","height":344}},"source":["import itertools\n","ModelObjectGrid.getModelDataframe(ModelObjectGrid.base_model_output,sort_column='test_precision',asscending=False,difference_by='f1_score',score_filter=['f1_score','precision','recall','zero_one_loss'])"],"execution_count":73,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-e8fa1faa6959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mModelObjectGrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetModelDataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelObjectGrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Difference_zero_one_loss_unit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0masscending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifference_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zero_one_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'zero_one_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-69-bcb7aa1597c1>\u001b[0m in \u001b[0;36mgetModelDataframe\u001b[0;34m(self, base_model_output, sort_column, asscending, difference_by, score_filter, agg)\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mscore_table\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mscore_filter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mscore_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0meachScore\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meachScore\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meachScore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_filter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mscore_columns_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mscore_columns_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-bcb7aa1597c1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mscore_table\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mscore_filter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mscore_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0meachScore\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meachScore\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meachScore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_filter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mscore_columns_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mscore_columns_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"]}]},{"cell_type":"code","metadata":{"id":"S_yJsyFpdPl_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"323f2dc4-1106-4eae-94d3-a441c0501bfb","executionInfo":{"status":"ok","timestamp":1566923642841,"user_tz":-330,"elapsed":1184,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["type(ModelObjectGrid.best_estimator_output['ExtraTreesClassifier'])"],"execution_count":129,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sklearn.ensemble.forest.ExtraTreesClassifier"]},"metadata":{"tags":[]},"execution_count":129}]},{"cell_type":"code","metadata":{"id":"IE0xCgaodnhd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a60f796d-c4a2-4d94-e807-c4abb198ff24","executionInfo":{"status":"ok","timestamp":1566931463963,"user_tz":-330,"elapsed":1178893,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["# TFIDF Grid Experiment with ExtraTrees \n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","text_clf = Pipeline([('vect', TfidfVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', ModelObjectGrid.best_estimator_output['ExtraTreesClassifier'])])\n","\n","# max_df=1.0, min_df=1\n","\n","tuned_parameters = {\n","    'tfidf__use_idf': (True, False),\n","    'tfidf__norm': ('l1', 'l2')\n","}\n","\n","X = df1['Clean_lemmed'][:train_data_index_last]\n","y = df1['sentiment'][:train_data_index_last]\n","\n","# from imblearn.over_sampling import SMOTE\n","# smt = SMOTE(random_state=777, k_neighbors=1)\n","# X_SMOTE, y_SMOTE = smt.fit_sample(X,y)\n","\n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","from sklearn.metrics import classification_report\n","clf = GridSearchCV(text_clf, tuned_parameters, cv=20, scoring=\"f1\",n_jobs=-1)\n","clf.fit(x_train,y_train)\n","\n","print(metrics.f1_score(y_test, clf.predict(x_test)))\n"],"execution_count":136,"outputs":[{"output_type":"stream","text":["0.5502392344497608\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fTM9NJ3e144k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"e2e84a25-e911-48db-e0c8-67a0b81aba81","executionInfo":{"status":"ok","timestamp":1566923545159,"user_tz":-330,"elapsed":1139,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["clf.best_estimator_"],"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 TfidfVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.float64'>,\n","                                 encoding='utf-8', input='content',\n","                                 lowercase=True, max_df=0.9, max_features=None,\n","                                 min_df=0.1, ngram_range=(1, 3), norm='l2',\n","                                 preprocessor=None, smooth_idf=True,\n","                                 stop_words=None, strip_accents=None,\n","                                 sublinear_tf=False,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, use_idf=True,\n","                                 vocabulary=None)),\n","                ('tfidf',\n","                 TfidfTransformer(norm='l2', smooth_idf=True,\n","                                  sublinear_tf=False, use_idf=True)),\n","                ('clf',\n","                 MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":127}]},{"cell_type":"code","metadata":{"id":"uu-EtKrP3FU4","colab_type":"code","colab":{}},"source":["#test data prep \n","test_data = df1[test_data_index_first:]\n","test_data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_zZYBZ4UEDq","colab_type":"code","outputId":"7440b038-b616-4951-93f9-674c4aeaacaf","colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"status":"ok","timestamp":1566920917234,"user_tz":-330,"elapsed":211168,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwG1VibMXTWbQ0ZQyb23IiuNGTb9GjO0tTjn2-ZA=s64","userId":"01031927567449846531"}}},"source":["# TPOT Experiment\n","\n","from tpot import TPOTClassifier\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_SMOTE.toarray(), y_SMOTE,\n","                                                    train_size=0.80, test_size=0.20)\n","\n","tpot = TPOTClassifier(generations=100, population_size=50, verbosity=3,scoring=\"f1\",n_jobs=-1)\n","tpot.fit(X_train, y_train)"],"execution_count":103,"outputs":[{"output_type":"stream","text":["30 operators have been imported by TPOT.\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3f77e468ecc4b959ecbbf98b9dc7efb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Optimization Progress', max=5050, style=ProgressStyle(descrip…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n","\r_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n","_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n","_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n","_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n","_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n","Generation 1 - Current Pareto front scores:\n","-1\t0.7389064192922963\tKNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=22, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n","\n","_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n","_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n","_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n","Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n","\n","\n","TPOT closed during evaluation in one generation.\n","WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n","\n","\n","TPOT closed prematurely. Will use the current best pipeline.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n","               disable_update_check=False, early_stop=None, generations=100,\n","               max_eval_time_mins=5, max_time_mins=None, memory=None,\n","               mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n","               periodic_checkpoint_folder=None, population_size=50,\n","               random_state=None, scoring='f1', subsample=1.0, template=None,\n","               use_dask=False, verbosity=3, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":103}]}]}