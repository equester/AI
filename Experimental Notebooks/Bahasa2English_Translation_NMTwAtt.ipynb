{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of nmt_with_attention.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb","timestamp":1563358662907}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CM1Cs9Xcdkv7","colab_type":"code","colab":{}},"source":["import pandas as pd\n","data_id = pd.read_csv('data_id.txt', sep=\"\\t\", header=None)\n","data_en = pd.read_csv('data_en.txt', sep=\"\\t\", header=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFtYhme0gfBW","colab_type":"code","colab":{}},"source":["data_final = data_id.merge(data_en,on=0,how=\"inner\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iClctgsg6cK","colab_type":"code","colab":{}},"source":["data_gt = pd.read_csv(\"g_translate_dump.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnpsyKQsiyKN","colab_type":"code","colab":{}},"source":["data_ind = pd.read_csv(\"ind.txt\",sep=\"\\t\",header=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OL9HGk7jGlJ","colab_type":"code","colab":{}},"source":["data_final.head()\n","df_final = data_final[['1_x','1_y']]\n","df_final.columns = [\"ind\",\"eng\"]\n","df_final.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZPW4vqihYUz","colab_type":"code","colab":{}},"source":["data_gt.head()\n","df_gt = data_gt[['raw_text','translated_text']]\n","df_gt.columns = [\"eng\",\"ind\"]\n","df_gt.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_hO-pyUj6U9","colab_type":"code","colab":{}},"source":["data_ind.head()\n","data_ind.columns = [\"eng\",\"ind\"]\n","data_ind.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqKAQ6IssR3F","colab_type":"code","colab":{}},"source":["FinalDF = data_ind.append(df_gt).append(df_final,sort=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXDwFUPRtMgu","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"U6lY35TJslGL","colab_type":"code","colab":{}},"source":["FinalDF.reset_index(inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eltp3BlJtVCE","colab_type":"code","colab":{}},"source":["del FinalDF['index']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kKiKu9OHsy5P","colab_type":"code","colab":{}},"source":["FinalDF.to_csv(\"FinalTranslationData.txt\",sep=\"\\t\",header=False,index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cc5BqZ0aeNmT","colab_type":"code","colab":{}},"source":["data_id.tail()\n","# len(data_id)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gx-cXMzbeYBT","colab_type":"code","colab":{}},"source":["data_en.tail()\n","# len(data_en)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"47Xa_WWCeT7F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tnxXKDjq3jEL","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","# Import TensorFlow >= 1.10 and enable eager execution\n","import tensorflow as tf\n","\n","tf.enable_eager_execution()\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import time\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kRVATYOgJs1b","colab":{}},"source":["# Download the file\n","# path_to_zip = tf.keras.utils.get_file(\n","#     'spa-eng.zip', origin='https://www.manythings.org/anki/ind-eng.zip', \n","#     extract=True)\n","\n","# path_to_file = os.path.dirname(path_to_zip)+\"/ind-eng/ind.txt\"\n","path_to_file = \"FinalTranslationData.txt\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rd0jw-eC3jEh","colab":{}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    \n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\" \n","    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    \n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    \n","    w = w.rstrip().strip()\n","    \n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    w = '<start> ' + w + ' <end>'\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OHn4Dct23jEm","colab":{}},"source":["# 1. Remove the accents\n","# 2. Clean the sentences\n","# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n","def create_dataset(path, num_examples):\n","    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n","    \n","    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines]\n","#     print(word_pairs)\n","    return word_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9xbqO7Iie9bb","colab":{}},"source":["# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n","# (e.g., 5 -> \"dad\") for each language,\n","class LanguageIndex():\n","  def __init__(self, lang):\n","    self.lang = lang\n","    self.word2idx = {}\n","    self.idx2word = {}\n","    self.vocab = set()\n","    \n","    self.create_index()\n","    \n","  def create_index(self):\n","    for phrase in self.lang:\n","      self.vocab.update(phrase.split(' '))\n","    \n","    self.vocab = sorted(self.vocab)\n","    \n","    self.word2idx['<pad>'] = 0\n","    for index, word in enumerate(self.vocab):\n","      self.word2idx[word] = index + 1\n","    \n","    for word, index in self.word2idx.items():\n","      self.idx2word[index] = word"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eAY9k49G3jE_","colab":{}},"source":["def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","\n","def load_dataset(path, num_examples):\n","    # creating cleaned input, output pairs\n","    pairs = create_dataset(path, num_examples)\n","    print(pairs)\n","    \n","    # index language using the class defined above    \n","    inp_lang = LanguageIndex(sp for en, sp in pairs)\n","    targ_lang = LanguageIndex(en for en, sp in pairs)\n","    \n","    # Vectorize the input and target languages\n","    \n","    # Spanish sentences\n","    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n","    \n","    # English sentences\n","    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n","    \n","    # Calculate max_length of input and output tensor\n","    # Here, we'll set those to the longest sentence in the dataset\n","    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n","    \n","    # Padding the input and output tensor to the maximum length\n","    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n","                                                                 maxlen=max_length_inp,\n","                                                                 padding='post')\n","    \n","    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n","                                                                  maxlen=max_length_tar, \n","                                                                  padding='post')\n","    \n","    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cnxC7q-j3jFD","colab":{}},"source":["# Try experimenting with the size of that dataset\n","num_examples = 10000\n","input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbcC6SUVwGMA","colab_type":"code","colab":{}},"source":["--NotebookApp.iopub_data_rate_limit=9000000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4QILQkOs3jFG","colab":{}},"source":["# Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","# Show length\n","len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rgCLkfv5uO3d"},"source":["### Create a tf.data dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TqHsArVZ3jFS","colab":{}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","N_BATCH = BUFFER_SIZE//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word2idx)\n","vocab_tar_size = len(targ_lang.word2idx)\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyZqgjin4NqL","colab_type":"code","colab":{}},"source":["dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"avyJ_4VIUoHb","colab":{}},"source":["def gru(units):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","  if tf.test.is_gpu_available():\n","    return tf.keras.layers.CuDNNGRU(units, \n","                                    return_sequences=True, \n","                                    return_state=True, \n","                                    recurrent_initializer='glorot_uniform')\n","  else:\n","    return tf.keras.layers.GRU(units, \n","                               return_sequences=True, \n","                               return_state=True, \n","                               recurrent_activation='sigmoid', \n","                               recurrent_initializer='glorot_uniform')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nZ2rI24i3jFg","colab":{}},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.enc_units)\n","        \n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)        \n","        return output, state\n","    \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yJ_B3mhW3jFk","colab":{}},"source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.dec_units)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        \n","        # used for attention\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, hidden, enc_output):\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        \n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n","        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","        \n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","        \n","        return x, state, attention_weights\n","        \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P5UY8wko3jFp","colab":{}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_ch_71VbIRfK"},"source":["## Define the optimizer and the loss function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WmTHr5iV3jFr","colab":{}},"source":["optimizer = tf.train.AdamOptimizer()\n","\n","\n","def loss_function(real, pred):\n","  mask = 1 - np.equal(real, 0)\n","  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n","  return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DMVWzzsfNl4e"},"source":["## Checkpoints (Object-based saving)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zj8bXQTgNwrF","colab":{}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hpObfY22IddU"},"source":["## Training\n","\n","1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n","2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n","3. The decoder returns the *predictions* and the *decoder hidden state*.\n","4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n","5. Use *teacher forcing* to decide the next input to the decoder.\n","6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n","7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ddefjBMa3jF0","colab":{}},"source":["EPOCHS = 50\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    \n","    hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    \n","    for (batch, (inp, targ)) in enumerate(dataset):\n","        loss = 0\n","        \n","        with tf.GradientTape() as tape:\n","            enc_output, enc_hidden = encoder(inp, hidden)\n","            \n","            dec_hidden = enc_hidden\n","            \n","            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n","            \n","            # Teacher forcing - feeding the target as the next input\n","            for t in range(1, targ.shape[1]):\n","                # passing enc_output to the decoder\n","                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","                \n","                loss += loss_function(targ[:, t], predictions)\n","                \n","                # using teacher forcing\n","                dec_input = tf.expand_dims(targ[:, t], 1)\n","        \n","        batch_loss = (loss / int(targ.shape[1]))\n","        \n","        total_loss += batch_loss\n","        \n","        variables = encoder.variables + decoder.variables\n","        \n","        gradients = tape.gradient(loss, variables)\n","        \n","        optimizer.apply_gradients(zip(gradients, variables))\n","        \n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                         batch,\n","                                                         batch_loss.numpy()))\n","    # saving (checkpoint) the model every 2 epochs\n","    if (epoch + 1) % 2 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","    \n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                        total_loss / N_BATCH))\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mU3Ce8M6I3rz"},"source":["## Translate\n","\n","* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n","* Stop predicting when the model predicts the *end token*.\n","* And store the *attention weights for every time step*.\n","\n","Note: The encoder output is calculated only once for one input."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EbQpyYs13jF_","colab":{}},"source":["def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    \n","    sentence = preprocess_sentence(sentence)\n","\n","    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n","        \n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.idx2word[predicted_id] + ' '\n","\n","        if targ_lang.idx2word[predicted_id] == '<end>':\n","            return result, sentence, attention_plot\n","        \n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result, sentence, attention_plot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s5hQWlbN3jGF","colab":{}},"source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap='viridis')\n","    \n","    fontdict = {'fontsize': 14}\n","    \n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sl9zUHzg3jGI","colab":{}},"source":["def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n","    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n","        \n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(result))\n","    \n","    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n250XbnjOaqP"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UJpT9D5_OgP6","colab":{}},"source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint_dir = './training_checkpoints'\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WrAM0FDomq3E","colab":{}},"source":["#Tom says this is absurd.\n","translate(u'Tom berkata ini tidak masuk akal.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zSx2iM36EZQZ","colab":{}},"source":["#We didn't learn a thing.\n","translate(u'Kita tidak mengambil hikmah sama sekali.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A3LLCx3ZE0Ls","colab":{}},"source":["#We take oil for granted.\n","translate(u'Selamat Hari untuk semua perempuan Indonesia di dunia', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DUQVLVqUE1YW","colab":{}},"source":["# wrong translation\n","translate(u'Lavi menanam sendiri semua sayuran yang ia makan?', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RTe5P5ioMJwN"},"source":["## Next steps\n","\n","* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n","* Experiment with training on a larger dataset, or using more epochs\n"]},{"cell_type":"code","metadata":{"id":"ZeNl5bjZMGke","colab_type":"code","colab":{}},"source":["!zip -r translationModel.zip training_checkpoints/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1g82DytbOC1","colab_type":"code","colab":{}},"source":["tpu_model.save_weights('./tpu_model.h5', overwrite=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dd27CpIPblSR","colab_type":"code","colab":{}},"source":["!ls -l --block-size=G"],"execution_count":0,"outputs":[]}]}