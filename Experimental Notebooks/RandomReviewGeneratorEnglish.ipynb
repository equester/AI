{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled54.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kh0KIH3TiuHK","colab_type":"code","colab":{}},"source":["# Install Libraries\n","import spacy\n","spacy.cli.download(\"en_core_web_md\")\n","!pip install rake_nltk\n","!pip install textacy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RekxqrZyivPQ","colab_type":"code","colab":{}},"source":["#Libraries \n","from flask import Flask, request\n","from itertools import islice\n","import pandas as pd\n","import numpy as np\n","import threading\n","import dill as pickle\n","import datetime\n","import random \n","import socket\n","import json\n","import gzip\n","import sys\n","import spacy\n","import tqdm\n","import time \n","import pandas as pd\n","from textblob import TextBlob\n","from rake_nltk import Metric, Rake\n","from textblob.np_extractors import ConllExtractor\n","extractor = ConllExtractor()\n","import textacy\n","import nltk\n","from nltk.corpus import stopwords \n","from dask import dataframe as dd\n","import textacy.keyterms\n","import pickle\n","\n","\n","\n","print(socket.gethostbyname(socket.getfqdn(socket.gethostname())))\n","\n","def getFeature(row):\n","    FeatureDict = {}\n","    # Rake Topic \n","    r = Rake(min_length=2, max_length=4) # Uses stopwords for english from NLTK, and all puntuation characters.\n","    r.extract_keywords_from_text(row)\n","    topics = r.get_ranked_phrases()\n","    \n","    text = nlp(row)\n","    # Noun Phrase \n","    \n","    noun_phrase_pairs = []\n","    for chunk in text.noun_chunks:\n","    #   print (chunk.text)\n","      if len(chunk.text.split(\" \"))>1 and chunk.text not in nltk.corpus.stopwords.words('english'):\n","        noun_phrase_pairs.append(chunk.text)\n","        \n","    # Sentiment \n","    Sentiment = float(\"{0:.2f}\".format(TextBlob(row).sentiment.polarity))\n","    \n","    # Negation Pairs \n","    Negation_Pair = []\n","    negation_tokens = [tok for tok in text if tok.dep_ == 'neg']\n","    negation_head_tokens = [token.head for token in negation_tokens]\n","    for token in negation_head_tokens:\n","      Negation_Pair.append([token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children]])\n","    \n","    # NMF Topics \n","      ## Not Now\n","      \n","    # Basic Entity Extraction \n","      ## Not now \n","    \n","    # Noun Adjective Pair ---> Rishi \n","    output = []\n","    a_n_pair = []\n","    \n","    n_a_pair = []\n","    for sent in text.sents: # sentences of a doc\n","      # This block handles adjective noun pairs\n","      for token in sent: # tokens of a sentence\n","        if token.pos_ == \"VERB\" : # since verbs are central connection between nouns and adjs\n","#           print(token, \"is a verb\")\n","            for child in token.children: # childen of verbs\n","#             print('child ', child)\n","              if child.pos_ in ['NOUN', 'PRON','PROPN']: \n","                for grand_child in child.children:\n","#                 print(\"grandchild\", grand_child)\n","                  if grand_child.pos_ in ['ADJ']:\n","                    a_n_pair.append(grand_child.string.strip()+\" \"+child.string.strip())\n","      if len(a_n_pair)>0:\n","        output.extend(a_n_pair)\n","      \n","      # This block handles noun adj pairs\n","      noun_flag = False\n","      noun = str\n","      adj = str()\n","      adj_flag = False\n","      for token in sent:\n","        if token.pos_ == \"VERB\" :\n","#           print(token, \"is a verb\")\n","            for child in token.children:\n","#             print('child2 ', child)\n","              if noun_flag == False:\n","                if child.pos_ in ['NOUN', 'PRON','PROPN']:\n","                  noun_flag = True\n","                  noun = child\n","                  continue\n","              if adj_flag == False:\n","                if child.pos_ in ['ADJ']:\n","                  adj_flag = True\n","                  adj = child\n","              if noun_flag and adj_flag:\n","                n_a_pair.append(adj.string.strip()+\" \"+noun.string.strip())\n","\n","      if len(n_a_pair)>0:\n","        output.extend(n_a_pair)\n","    output = list(set(output))\n","    \n","    # Subject Verb Object \n","    SVOPair = list(textacy.extract.subject_verb_object_triples(text))\n","    # SG Rank \n","    SGRankTopic = list(textacy.keyterms.sgrank(text, ngrams=(1, 2, 3, 4, 5, 6), normalize='lemma', window_width=1500, n_keyterms=10, idf=None))\n","    # TextRank \n","    TextRank = list(textacy.keyterms.textrank(text, normalize='lemma', n_keyterms=10))\n","    # Key Terms Semantic Network \n","    KeyTermPageRank = list(textacy.keyterms.key_terms_from_semantic_network(text, normalize='lemma', window_width=2, edge_weighting='binary', ranking_algo='pagerank', join_key_words=False, n_keyterms=5))\n","    \n","    # Topic from NMF \n","    FeatureDict['RakeTopics'] = topics\n","    FeatureDict['NounPhrase'] = noun_phrase_pairs\n","    FeatureDict['Sentiment'] = Sentiment\n","    FeatureDict['Negation_Pair'] = Negation_Pair\n","    FeatureDict['SubjectVerbObject'] = SVOPair\n","    FeatureDict['SGRankTopic'] = SGRankTopic\n","    FeatureDict['TextRank'] = TextRank\n","    FeatureDict['PageRankTopic'] = KeyTermPageRank\n","    FeatureDict['NounAdjectivePair'] = output\n","    FeatureDict['SPacyDocObject'] = [nlp(row)]\n","    return FeatureDict\n","  \n","def getDocDump(row,nlp):\n","  NLPObject = {}\n","  NLPObject['Object'] = [nlp(row)]\n","  return NLPObject\n","\n","\n","app = Flask(\"GenerateRandomReviews\")\n","@app.route('/TextFeatureDump')\n","def ExtractReviews():\n","  \"\"\"\n","  Takes DataFrame --> NPZ to DF \n","  \n","  \"\"\"\n","#   DF = request.args.get('DFLocation')\n","  \n","  global nlp\n","  nlp = spacy.load(\"en_core_web_md\")\n","#   nlp = spacy.load(\"en\")\n","  data = pd.read_json(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz\", lines=True)\n","\n","  data = data.head(10)\n","  ddata = dd.from_pandas(data, npartitions=4)\n","#   %time ddata['DocObject']  = ddata.map_partitions(lambda df: df.reviewText.apply((lambda row: getDocDump(row,nlp)))).compute(scheduler='threads')\n","  %time ddata['ReviewFeature'] = ddata.map_partitions(lambda df: df.reviewText.apply((lambda row: getFeature(row)))).compute(scheduler='synchronous', optimize_graph='True')\n","  df = ddata.compute()\n","  \n","#   df.to_pickle(\"TestPickle.pickle\")\n","  df.to_csv(\"TestAPI.csv\")\n","#   with open(\"backup\", 'wb') as f:\n","#     pickle.dump(df, f)\n","#   df.to_pickle(\"Test_Pickle.pickle\")\n","#   dfjson = df.to_json()\n","#   with open('data.txt', 'w') as outfile:  \n","#     json.dumps(dfjson)\n","#   outfile.close()\n","  \n","  \n","#   return (df.to_json(orient='records'))\n","  return (\"DOne\")\n","\n","\n","\n","\n","threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':}).start()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXw86O66i3ml","colab_type":"code","colab":{}},"source":["#Libraries \n","from flask import Flask, request\n","from itertools import islice\n","import pandas as pd\n","import numpy as np\n","import threading\n","import dill as pickle\n","import datetime\n","import random \n","import socket\n","import json\n","import gzip\n","import sys\n","import spacy\n","import tqdm\n","import time \n","import pandas as pd\n","from textblob import TextBlob\n","from rake_nltk import Metric, Rake\n","from textblob.np_extractors import ConllExtractor\n","extractor = ConllExtractor()\n","import textacy\n","import nltk\n","from nltk.corpus import stopwords \n","from dask import dataframe as dd\n","import textacy.keyterms\n","import pickle\n","\n","\n","\n","print(socket.gethostbyname(socket.getfqdn(socket.gethostname())))\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","app = Flask(\"GenerateRandomReviews\")\n","@app.route('/TextFeatureDump')\n","def ExtractReviews():\n","  \"\"\"\n","  Takes DataFrame --> NPZ to DF \n","  \n","  \"\"\"\n","#   DF = request.args.get('DFLocation')\n","  \n","\n","  \n","  \n","  \n","  \n","  global nlp\n","  nlp = spacy.load(\"en_core_web_md\")\n","#   nlp = spacy.load(\"en\")\n","  data = pd.read_json(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz\", lines=True)\n","\n","  data = data.head(10)\n","  ddata = dd.from_pandas(data, npartitions=4)\n","#   %time ddata['DocObject']  = ddata.map_partitions(lambda df: df.reviewText.apply((lambda row: getDocDump(row,nlp)))).compute(scheduler='threads')\n","  %time ddata['ReviewFeature'] = ddata.map_partitions(lambda df: df.reviewText.apply((lambda row: getFeature(row)))).compute(scheduler='synchronous', optimize_graph='True')\n","  df = ddata.compute()\n","  \n","#   df.to_pickle(\"TestPickle.pickle\")\n","  df.to_csv(\"TestAPI.csv\")\n","#   with open(\"backup\", 'wb') as f:\n","#     pickle.dump(df, f)\n","#   df.to_pickle(\"Test_Pickle.pickle\")\n","#   dfjson = df.to_json()\n","#   with open('data.txt', 'w') as outfile:  \n","#     json.dumps(dfjson)\n","#   outfile.close()\n","  \n","  \n","#   return (df.to_json(orient='records'))\n","  return (\"DOne\")\n","\n","\n","\n","\n","threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':}).start()\n","\n"],"execution_count":0,"outputs":[]}]}