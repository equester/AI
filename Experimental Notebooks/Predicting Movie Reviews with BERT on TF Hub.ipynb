{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Predicting Movie Reviews with BERT on TF Hub.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1555941235309}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"cell_type":"markdown","source":["#Predicting Movie Review Sentiment with BERT on TF Hub"]},{"metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"cell_type":"markdown","source":["If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n","\n","Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n","\n","Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"]},{"metadata":{"id":"hsZvic2YxnTz","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cp5wfXDx5SPH","colab_type":"text"},"cell_type":"markdown","source":["In addition to the standard libraries we imported above, we'll need to install BERT's python package."]},{"metadata":{"id":"jviywGyWyKsA","colab_type":"code","outputId":"8bf1927d-d999-4b9d-a70b-293100da1bb9","executionInfo":{"status":"ok","timestamp":1555983458256,"user_tz":-330,"elapsed":6247,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["!pip install bert-tensorflow"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"hhbGEfwgdEtw","colab_type":"code","colab":{}},"cell_type":"code","source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"cell_type":"markdown","source":["Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n","\n","Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"metadata":{"id":"US_EAnICvP7f","colab_type":"code","outputId":"5bb9cc81-e80f-4b4d-ca3c-63b4c244ba7d","cellView":"form","executionInfo":{"status":"ok","timestamp":1555983466099,"user_tz":-330,"elapsed":977,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'bert_model'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = False #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = False #@param {type:\"boolean\"}\n","BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["***** Model output directory: bert_model *****\n"],"name":"stdout"}]},{"metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"cell_type":"markdown","source":["#Data"]},{"metadata":{"id":"MC_w8SRqN0fr","colab_type":"text"},"cell_type":"markdown","source":["First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."]},{"metadata":{"id":"fom_ff20gyy6","colab_type":"code","colab":{}},"cell_type":"code","source":["from tensorflow import keras\n","import os\n","import re\n","\n","# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"aclImdb.tar.gz\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","  \n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"aclImdb\", \"test\"))\n","  \n","  return train_df, test_df\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2abfwdn-g135","colab_type":"code","colab":{}},"cell_type":"code","source":["train, test = download_and_load_datasets()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XA8WHJgzhIZf","colab_type":"text"},"cell_type":"markdown","source":["To keep training fast, we'll take a sample of 5000 train and test examples, respectively."]},{"metadata":{"id":"lw_F488eixTV","colab_type":"code","colab":{}},"cell_type":"code","source":["train = train.sample(5000)\n","test = test.sample(5000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"prRQM8pDi8xI","colab_type":"code","outputId":"9cb4c911-d9e9-4dc8-948d-c2c3ea1af771","executionInfo":{"status":"ok","timestamp":1555983587172,"user_tz":-330,"elapsed":62224,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["train.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['sentence', 'sentiment', 'polarity'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"sfRnHSz3iSXz","colab_type":"text"},"cell_type":"markdown","source":["For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"]},{"metadata":{"id":"IuMOGwFui4it","colab_type":"code","colab":{}},"cell_type":"code","source":["DATA_COLUMN = 'sentence'\n","LABEL_COLUMN = 'polarity'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pzge0J5XVQZs","colab_type":"code","outputId":"03c93454-f5ab-487f-ee50-961bbb637c37","executionInfo":{"status":"ok","timestamp":1555983587176,"user_tz":-330,"elapsed":47457,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["label_list"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1]"]},"metadata":{"tags":[]},"execution_count":35}]},{"metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"cell_type":"markdown","source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"cell_type":"markdown","source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Lowercase our text (if we're using a BERT lowercase model)\n","2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","4. Map our words to indexes using a vocab file that BERT provides\n","5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details.\n","\n","\n"]},{"metadata":{"id":"qMWiDtpyQSoU","colab_type":"text"},"cell_type":"markdown","source":["To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"]},{"metadata":{"id":"IhJSe0QHNG7U","colab_type":"code","outputId":"a0298944-0350-4d01-a9be-78d9e20cb725","executionInfo":{"status":"ok","timestamp":1555983608237,"user_tz":-330,"elapsed":4691,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:15.903031 139939410937728 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"metadata":{"id":"z4oFkhpZBDKm","colab_type":"text"},"cell_type":"markdown","source":["Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"]},{"metadata":{"id":"dsBo6RCtQmwx","colab_type":"code","outputId":"0a825380-4dda-4ab8-dd0a-606d26b6dbed","executionInfo":{"status":"ok","timestamp":1555983608238,"user_tz":-330,"elapsed":2349,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"cell_type":"code","source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"cell_type":"markdown","source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"metadata":{"id":"ScryO3Hz2bW_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LL5W8gEGRTAf","colab_type":"code","outputId":"18a3094a-504f-4a4d-f6b2-2cf86e14d856","executionInfo":{"status":"ok","timestamp":1555983669155,"user_tz":-330,"elapsed":54822,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":2485}},"cell_type":"code","source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.299641 139939410937728 run_classifier.py:774] Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.323042 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.325448 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i couldn ' t help but feel that this could have been a bigger movie than it was . the screenplay is highly intelligent and it just seemed that it could have been opened up in a way more reminiscent of seven . not by changing the story - i think mainly through the cinematography . the cinematography was the only thing that i found to be holding back the film . on the other hand , the pacing was absolutely on point . whoever worked on the editing really did their job well . and i thought bill paxton did a great job of directing . now away from the technical stuff . . . < br / > < br / > this movie [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.327732 139939410937728 run_classifier.py:464] tokens: [CLS] i couldn ' t help but feel that this could have been a bigger movie than it was . the screenplay is highly intelligent and it just seemed that it could have been opened up in a way more reminiscent of seven . not by changing the story - i think mainly through the cinematography . the cinematography was the only thing that i found to be holding back the film . on the other hand , the pacing was absolutely on point . whoever worked on the editing really did their job well . and i thought bill paxton did a great job of directing . now away from the technical stuff . . . < br / > < br / > this movie [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2481 1005 1056 2393 2021 2514 2008 2023 2071 2031 2042 1037 7046 3185 2084 2009 2001 1012 1996 9000 2003 3811 9414 1998 2009 2074 2790 2008 2009 2071 2031 2042 2441 2039 1999 1037 2126 2062 14563 1997 2698 1012 2025 2011 5278 1996 2466 1011 1045 2228 3701 2083 1996 16434 1012 1996 16434 2001 1996 2069 2518 2008 1045 2179 2000 2022 3173 2067 1996 2143 1012 2006 1996 2060 2192 1010 1996 15732 2001 7078 2006 2391 1012 9444 2499 2006 1996 9260 2428 2106 2037 3105 2092 1012 1998 1045 2245 3021 27765 2106 1037 2307 3105 1997 9855 1012 2085 2185 2013 1996 4087 4933 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 3185 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.329967 139939410937728 run_classifier.py:465] input_ids: 101 1045 2481 1005 1056 2393 2021 2514 2008 2023 2071 2031 2042 1037 7046 3185 2084 2009 2001 1012 1996 9000 2003 3811 9414 1998 2009 2074 2790 2008 2009 2071 2031 2042 2441 2039 1999 1037 2126 2062 14563 1997 2698 1012 2025 2011 5278 1996 2466 1011 1045 2228 3701 2083 1996 16434 1012 1996 16434 2001 1996 2069 2518 2008 1045 2179 2000 2022 3173 2067 1996 2143 1012 2006 1996 2060 2192 1010 1996 15732 2001 7078 2006 2391 1012 9444 2499 2006 1996 9260 2428 2106 2037 3105 2092 1012 1998 1045 2245 3021 27765 2106 1037 2307 3105 1997 9855 1012 2085 2185 2013 1996 4087 4933 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 3185 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.333697 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.335956 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.340312 139939410937728 run_classifier.py:468] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.365459 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.367729 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this worldwide was the cheap man ' s version of what the nwa under jim cr ##ock ##ett junior and jim cr ##ock ##ett promotions made back in the 1980s on the localized \" big 3 \" stations during the saturday morning / afternoon wrestling cr ##az ##e . when ted turner got his hands on cr ##ock ##ett ' s failed version of nwa he turned it into world championship wrestling and proceeded to drop all nwa references all together . nwa world wide and nwa pro wrestling were re ##lab ##ele ##d with the wcw logo and moved off the road to disney / mgm studios in orlando , florida and eventually became nothing more than rec ##ap shows for wcw ' s ni [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.369718 139939410937728 run_classifier.py:464] tokens: [CLS] this worldwide was the cheap man ' s version of what the nwa under jim cr ##ock ##ett junior and jim cr ##ock ##ett promotions made back in the 1980s on the localized \" big 3 \" stations during the saturday morning / afternoon wrestling cr ##az ##e . when ted turner got his hands on cr ##ock ##ett ' s failed version of nwa he turned it into world championship wrestling and proceeded to drop all nwa references all together . nwa world wide and nwa pro wrestling were re ##lab ##ele ##d with the wcw logo and moved off the road to disney / mgm studios in orlando , florida and eventually became nothing more than rec ##ap shows for wcw ' s ni [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 4969 2001 1996 10036 2158 1005 1055 2544 1997 2054 1996 15737 2104 3958 13675 7432 6582 3502 1998 3958 13675 7432 6582 15365 2081 2067 1999 1996 3865 2006 1996 22574 1000 2502 1017 1000 3703 2076 1996 5095 2851 1013 5027 4843 13675 10936 2063 1012 2043 6945 6769 2288 2010 2398 2006 13675 7432 6582 1005 1055 3478 2544 1997 15737 2002 2357 2009 2046 2088 2528 4843 1998 8979 2000 4530 2035 15737 7604 2035 2362 1012 15737 2088 2898 1998 15737 4013 4843 2020 2128 20470 12260 2094 2007 1996 24215 8154 1998 2333 2125 1996 2346 2000 6373 1013 15418 4835 1999 10108 1010 3516 1998 2776 2150 2498 2062 2084 28667 9331 3065 2005 24215 1005 1055 9152 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.371966 139939410937728 run_classifier.py:465] input_ids: 101 2023 4969 2001 1996 10036 2158 1005 1055 2544 1997 2054 1996 15737 2104 3958 13675 7432 6582 3502 1998 3958 13675 7432 6582 15365 2081 2067 1999 1996 3865 2006 1996 22574 1000 2502 1017 1000 3703 2076 1996 5095 2851 1013 5027 4843 13675 10936 2063 1012 2043 6945 6769 2288 2010 2398 2006 13675 7432 6582 1005 1055 3478 2544 1997 15737 2002 2357 2009 2046 2088 2528 4843 1998 8979 2000 4530 2035 15737 7604 2035 2362 1012 15737 2088 2898 1998 15737 4013 4843 2020 2128 20470 12260 2094 2007 1996 24215 8154 1998 2333 2125 1996 2346 2000 6373 1013 15418 4835 1999 10108 1010 3516 1998 2776 2150 2498 2062 2084 28667 9331 3065 2005 24215 1005 1055 9152 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.374186 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.376239 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.378208 139939410937728 run_classifier.py:468] label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.389787 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.391996 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] if the lion king was a disney version of hamlet , then the lion king 3 : ha ##ku ##na mata ##ta is a disney version of guild ##ens ##tern and rosen ##cr ##ant ##z are dead . just like tom stop ##par ##d ' s beg ##uil ##ing film , we get to view the action from the point of view of two of the minor characters from the original : tim ##on , the me ##er ##kat with a pen ##chan ##t for breaking into song at the drop of a hat , and pu ##mba ##a , the war ##th ##og with flat ##ule ##nce issues . by following their story - rather than sim ##ba ' s - we get to see [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.394224 139939410937728 run_classifier.py:464] tokens: [CLS] if the lion king was a disney version of hamlet , then the lion king 3 : ha ##ku ##na mata ##ta is a disney version of guild ##ens ##tern and rosen ##cr ##ant ##z are dead . just like tom stop ##par ##d ' s beg ##uil ##ing film , we get to view the action from the point of view of two of the minor characters from the original : tim ##on , the me ##er ##kat with a pen ##chan ##t for breaking into song at the drop of a hat , and pu ##mba ##a , the war ##th ##og with flat ##ule ##nce issues . by following their story - rather than sim ##ba ' s - we get to see [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2065 1996 7006 2332 2001 1037 6373 2544 1997 8429 1010 2059 1996 7006 2332 1017 1024 5292 5283 2532 22640 2696 2003 1037 6373 2544 1997 9054 6132 16451 1998 21701 26775 4630 2480 2024 2757 1012 2074 2066 3419 2644 19362 2094 1005 1055 11693 19231 2075 2143 1010 2057 2131 2000 3193 1996 2895 2013 1996 2391 1997 3193 1997 2048 1997 1996 3576 3494 2013 1996 2434 1024 5199 2239 1010 1996 2033 2121 24498 2007 1037 7279 14856 2102 2005 4911 2046 2299 2012 1996 4530 1997 1037 6045 1010 1998 16405 11201 2050 1010 1996 2162 2705 8649 2007 4257 9307 5897 3314 1012 2011 2206 2037 2466 1011 2738 2084 21934 3676 1005 1055 1011 2057 2131 2000 2156 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.396430 139939410937728 run_classifier.py:465] input_ids: 101 2065 1996 7006 2332 2001 1037 6373 2544 1997 8429 1010 2059 1996 7006 2332 1017 1024 5292 5283 2532 22640 2696 2003 1037 6373 2544 1997 9054 6132 16451 1998 21701 26775 4630 2480 2024 2757 1012 2074 2066 3419 2644 19362 2094 1005 1055 11693 19231 2075 2143 1010 2057 2131 2000 3193 1996 2895 2013 1996 2391 1997 3193 1997 2048 1997 1996 3576 3494 2013 1996 2434 1024 5199 2239 1010 1996 2033 2121 24498 2007 1037 7279 14856 2102 2005 4911 2046 2299 2012 1996 4530 1997 1037 6045 1010 1998 16405 11201 2050 1010 1996 2162 2705 8649 2007 4257 9307 5897 3314 1012 2011 2206 2037 2466 1011 2738 2084 21934 3676 1005 1055 1011 2057 2131 2000 2156 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.398580 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.400805 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.402539 139939410937728 run_classifier.py:468] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.413723 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.416001 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] although i was born in the year that this movie came out and had never heard of it until my junior year of high school ( 1996 ) when i saw it i became totally eng ##ross ##ed laughing and crying and feeling along with the characters because me and my friends were them . < br / > < br / > their hair , clothes and speech were outdated but the emotions and the desperation of each situation were so familiar ! i remember thinking how real it was and how i wished that they would make movies like that still . < br / > < br / > in fact i saw this movie the night after i had been at a [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.418097 139939410937728 run_classifier.py:464] tokens: [CLS] although i was born in the year that this movie came out and had never heard of it until my junior year of high school ( 1996 ) when i saw it i became totally eng ##ross ##ed laughing and crying and feeling along with the characters because me and my friends were them . < br / > < br / > their hair , clothes and speech were outdated but the emotions and the desperation of each situation were so familiar ! i remember thinking how real it was and how i wished that they would make movies like that still . < br / > < br / > in fact i saw this movie the night after i had been at a [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2348 1045 2001 2141 1999 1996 2095 2008 2023 3185 2234 2041 1998 2018 2196 2657 1997 2009 2127 2026 3502 2095 1997 2152 2082 1006 2727 1007 2043 1045 2387 2009 1045 2150 6135 25540 25725 2098 5870 1998 6933 1998 3110 2247 2007 1996 3494 2138 2033 1998 2026 2814 2020 2068 1012 1026 7987 1013 1028 1026 7987 1013 1028 2037 2606 1010 4253 1998 4613 2020 25963 2021 1996 6699 1998 1996 15561 1997 2169 3663 2020 2061 5220 999 1045 3342 3241 2129 2613 2009 2001 1998 2129 1045 6257 2008 2027 2052 2191 5691 2066 2008 2145 1012 1026 7987 1013 1028 1026 7987 1013 1028 1999 2755 1045 2387 2023 3185 1996 2305 2044 1045 2018 2042 2012 1037 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.420334 139939410937728 run_classifier.py:465] input_ids: 101 2348 1045 2001 2141 1999 1996 2095 2008 2023 3185 2234 2041 1998 2018 2196 2657 1997 2009 2127 2026 3502 2095 1997 2152 2082 1006 2727 1007 2043 1045 2387 2009 1045 2150 6135 25540 25725 2098 5870 1998 6933 1998 3110 2247 2007 1996 3494 2138 2033 1998 2026 2814 2020 2068 1012 1026 7987 1013 1028 1026 7987 1013 1028 2037 2606 1010 4253 1998 4613 2020 25963 2021 1996 6699 1998 1996 15561 1997 2169 3663 2020 2061 5220 999 1045 3342 3241 2129 2613 2009 2001 1998 2129 1045 6257 2008 2027 2052 2191 5691 2066 2008 2145 1012 1026 7987 1013 1028 1026 7987 1013 1028 1999 2755 1045 2387 2023 3185 1996 2305 2044 1045 2018 2042 2012 1037 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.422428 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.424573 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.426503 139939410937728 run_classifier.py:468] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.432688 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.437405 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i am dumb ##founded that i actually sat and watched this . i love independent films , horror films , and the whole zombie thing in general . but when you add ni ##nga ' s , you ' ve crossed a line that should never be crossed . i hope the people in this movie had a great time making it , then at least it wasn ' t a total waste . you ' d never know by watching it though . script ? are you kidding . acting ? i think even the trees were fa ##king . cinematography ? well , there must ' ve been a camera there . period . i don ' t think there was any actual planning [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.439420 139939410937728 run_classifier.py:464] tokens: [CLS] i am dumb ##founded that i actually sat and watched this . i love independent films , horror films , and the whole zombie thing in general . but when you add ni ##nga ' s , you ' ve crossed a line that should never be crossed . i hope the people in this movie had a great time making it , then at least it wasn ' t a total waste . you ' d never know by watching it though . script ? are you kidding . acting ? i think even the trees were fa ##king . cinematography ? well , there must ' ve been a camera there . period . i don ' t think there was any actual planning [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2572 12873 21001 2008 1045 2941 2938 1998 3427 2023 1012 1045 2293 2981 3152 1010 5469 3152 1010 1998 1996 2878 11798 2518 1999 2236 1012 2021 2043 2017 5587 9152 13807 1005 1055 1010 2017 1005 2310 4625 1037 2240 2008 2323 2196 2022 4625 1012 1045 3246 1996 2111 1999 2023 3185 2018 1037 2307 2051 2437 2009 1010 2059 2012 2560 2009 2347 1005 1056 1037 2561 5949 1012 2017 1005 1040 2196 2113 2011 3666 2009 2295 1012 5896 1029 2024 2017 12489 1012 3772 1029 1045 2228 2130 1996 3628 2020 6904 6834 1012 16434 1029 2092 1010 2045 2442 1005 2310 2042 1037 4950 2045 1012 2558 1012 1045 2123 1005 1056 2228 2045 2001 2151 5025 4041 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.441426 139939410937728 run_classifier.py:465] input_ids: 101 1045 2572 12873 21001 2008 1045 2941 2938 1998 3427 2023 1012 1045 2293 2981 3152 1010 5469 3152 1010 1998 1996 2878 11798 2518 1999 2236 1012 2021 2043 2017 5587 9152 13807 1005 1055 1010 2017 1005 2310 4625 1037 2240 2008 2323 2196 2022 4625 1012 1045 3246 1996 2111 1999 2023 3185 2018 1037 2307 2051 2437 2009 1010 2059 2012 2560 2009 2347 1005 1056 1037 2561 5949 1012 2017 1005 1040 2196 2113 2011 3666 2009 2295 1012 5896 1029 2024 2017 12489 1012 3772 1029 1045 2228 2130 1996 3628 2020 6904 6834 1012 16434 1029 2092 1010 2045 2442 1005 2310 2042 1037 4950 2045 1012 2558 1012 1045 2123 1005 1056 2228 2045 2001 2151 5025 4041 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.443494 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.445572 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:23.447509 139939410937728 run_classifier.py:468] label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.477196 139939410937728 run_classifier.py:774] Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.485968 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.493386 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] fa ##q ##rs ##cape is truly one of those shows that just has it all , great acting , great cast , great writing , sets , chemistry , mu ##ppet ##s . . . it ' s got it all and then some , except a home . this fantastic series it ' s seem has it all except and ending . t ##pt ##b seem to think this is a series that is consecutive single set shows , when anyone who watches know this is an ongoing , one epic , love story , that has an end that must been seen . if you have never watched far ##sca ##pe do you ##sel ##f a favor and check it out on dvd when [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.503009 139939410937728 run_classifier.py:464] tokens: [CLS] fa ##q ##rs ##cape is truly one of those shows that just has it all , great acting , great cast , great writing , sets , chemistry , mu ##ppet ##s . . . it ' s got it all and then some , except a home . this fantastic series it ' s seem has it all except and ending . t ##pt ##b seem to think this is a series that is consecutive single set shows , when anyone who watches know this is an ongoing , one epic , love story , that has an end that must been seen . if you have never watched far ##sca ##pe do you ##sel ##f a favor and check it out on dvd when [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 6904 4160 2869 19464 2003 5621 2028 1997 2216 3065 2008 2074 2038 2009 2035 1010 2307 3772 1010 2307 3459 1010 2307 3015 1010 4520 1010 6370 1010 14163 29519 2015 1012 1012 1012 2009 1005 1055 2288 2009 2035 1998 2059 2070 1010 3272 1037 2188 1012 2023 10392 2186 2009 1005 1055 4025 2038 2009 2035 3272 1998 4566 1012 1056 13876 2497 4025 2000 2228 2023 2003 1037 2186 2008 2003 5486 2309 2275 3065 1010 2043 3087 2040 12197 2113 2023 2003 2019 7552 1010 2028 8680 1010 2293 2466 1010 2008 2038 2019 2203 2008 2442 2042 2464 1012 2065 2017 2031 2196 3427 2521 15782 5051 2079 2017 11246 2546 1037 5684 1998 4638 2009 2041 2006 4966 2043 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.505078 139939410937728 run_classifier.py:465] input_ids: 101 6904 4160 2869 19464 2003 5621 2028 1997 2216 3065 2008 2074 2038 2009 2035 1010 2307 3772 1010 2307 3459 1010 2307 3015 1010 4520 1010 6370 1010 14163 29519 2015 1012 1012 1012 2009 1005 1055 2288 2009 2035 1998 2059 2070 1010 3272 1037 2188 1012 2023 10392 2186 2009 1005 1055 4025 2038 2009 2035 3272 1998 4566 1012 1056 13876 2497 4025 2000 2228 2023 2003 1037 2186 2008 2003 5486 2309 2275 3065 1010 2043 3087 2040 12197 2113 2023 2003 2019 7552 1010 2028 8680 1010 2293 2466 1010 2008 2038 2019 2203 2008 2442 2042 2464 1012 2065 2017 2031 2196 3427 2521 15782 5051 2079 2017 11246 2546 1037 5684 1998 4638 2009 2041 2006 4966 2043 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.507058 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.510495 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.512248 139939410937728 run_classifier.py:468] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.527501 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.529640 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this bela ##bor ##ed and sloppy spy mel ##od ##rama featuring two buff ##oon ##ish ( one ideal ##istic , one drug add ##led ) california kids dealing secrets to the kgb never seems to get enough steam up to sustain any tension and suspense before it dies a very slow death over two hours later . john sc ##hl ##es ##inger ' s finished product gives the impression that he was asleep in his director ' s chair most of the time as the film la ##gs and the actors sleep walk , save for the highly annoying over the top performance of sean penn . < br / > < br / > childhood altar boys and friends chris ( tim hutton ) and [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.531714 139939410937728 run_classifier.py:464] tokens: [CLS] this bela ##bor ##ed and sloppy spy mel ##od ##rama featuring two buff ##oon ##ish ( one ideal ##istic , one drug add ##led ) california kids dealing secrets to the kgb never seems to get enough steam up to sustain any tension and suspense before it dies a very slow death over two hours later . john sc ##hl ##es ##inger ' s finished product gives the impression that he was asleep in his director ' s chair most of the time as the film la ##gs and the actors sleep walk , save for the highly annoying over the top performance of sean penn . < br / > < br / > childhood altar boys and friends chris ( tim hutton ) and [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 20252 12821 2098 1998 28810 8645 11463 7716 14672 3794 2048 23176 7828 4509 1006 2028 7812 6553 1010 2028 4319 5587 3709 1007 2662 4268 7149 7800 2000 1996 25467 2196 3849 2000 2131 2438 5492 2039 2000 15770 2151 6980 1998 23873 2077 2009 8289 1037 2200 4030 2331 2058 2048 2847 2101 1012 2198 8040 7317 2229 9912 1005 1055 2736 4031 3957 1996 8605 2008 2002 2001 6680 1999 2010 2472 1005 1055 3242 2087 1997 1996 2051 2004 1996 2143 2474 5620 1998 1996 5889 3637 3328 1010 3828 2005 1996 3811 15703 2058 1996 2327 2836 1997 5977 9502 1012 1026 7987 1013 1028 1026 7987 1013 1028 5593 9216 3337 1998 2814 3782 1006 5199 20408 1007 1998 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.534307 139939410937728 run_classifier.py:465] input_ids: 101 2023 20252 12821 2098 1998 28810 8645 11463 7716 14672 3794 2048 23176 7828 4509 1006 2028 7812 6553 1010 2028 4319 5587 3709 1007 2662 4268 7149 7800 2000 1996 25467 2196 3849 2000 2131 2438 5492 2039 2000 15770 2151 6980 1998 23873 2077 2009 8289 1037 2200 4030 2331 2058 2048 2847 2101 1012 2198 8040 7317 2229 9912 1005 1055 2736 4031 3957 1996 8605 2008 2002 2001 6680 1999 2010 2472 1005 1055 3242 2087 1997 1996 2051 2004 1996 2143 2474 5620 1998 1996 5889 3637 3328 1010 3828 2005 1996 3811 15703 2058 1996 2327 2836 1997 5977 9502 1012 1026 7987 1013 1028 1026 7987 1013 1028 5593 9216 3337 1998 2814 3782 1006 5199 20408 1007 1998 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.536420 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.538246 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.540370 139939410937728 run_classifier.py:468] label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.558853 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.560716 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] walter matt ##ha ##u is wonderful as the \" phil ##ander ##ing \" dentist dr . julian winston whose frequent fi ##bs to girlfriend gold ##ie provide textbook proof of the dangers of lying . gold ##ie ha ##wn ' s touching ko ##ok toni simmons certainly deserved to win her oscar . ingrid bergman ' s work as the stiff - as - star ##ch nurse stephanie is also touching to watch as she comes out of her shell , slowly and nervously . this is a great movie to watch in the spring ##time , or any time for that matter . it ' s very under ##rated ; i never heard about it until i found it in the video store , and [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.562681 139939410937728 run_classifier.py:464] tokens: [CLS] walter matt ##ha ##u is wonderful as the \" phil ##ander ##ing \" dentist dr . julian winston whose frequent fi ##bs to girlfriend gold ##ie provide textbook proof of the dangers of lying . gold ##ie ha ##wn ' s touching ko ##ok toni simmons certainly deserved to win her oscar . ingrid bergman ' s work as the stiff - as - star ##ch nurse stephanie is also touching to watch as she comes out of her shell , slowly and nervously . this is a great movie to watch in the spring ##time , or any time for that matter . it ' s very under ##rated ; i never heard about it until i found it in the video store , and [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4787 4717 3270 2226 2003 6919 2004 1996 1000 6316 12243 2075 1000 24385 2852 1012 6426 10180 3005 6976 10882 5910 2000 6513 2751 2666 3073 16432 6947 1997 1996 16796 1997 4688 1012 2751 2666 5292 7962 1005 1055 7244 12849 6559 16525 13672 5121 10849 2000 2663 2014 7436 1012 22093 24544 1005 1055 2147 2004 1996 10551 1011 2004 1011 2732 2818 6821 11496 2003 2036 7244 2000 3422 2004 2016 3310 2041 1997 2014 5806 1010 3254 1998 12531 1012 2023 2003 1037 2307 3185 2000 3422 1999 1996 3500 7292 1010 2030 2151 2051 2005 2008 3043 1012 2009 1005 1055 2200 2104 9250 1025 1045 2196 2657 2055 2009 2127 1045 2179 2009 1999 1996 2678 3573 1010 1998 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.564645 139939410937728 run_classifier.py:465] input_ids: 101 4787 4717 3270 2226 2003 6919 2004 1996 1000 6316 12243 2075 1000 24385 2852 1012 6426 10180 3005 6976 10882 5910 2000 6513 2751 2666 3073 16432 6947 1997 1996 16796 1997 4688 1012 2751 2666 5292 7962 1005 1055 7244 12849 6559 16525 13672 5121 10849 2000 2663 2014 7436 1012 22093 24544 1005 1055 2147 2004 1996 10551 1011 2004 1011 2732 2818 6821 11496 2003 2036 7244 2000 3422 2004 2016 3310 2041 1997 2014 5806 1010 3254 1998 12531 1012 2023 2003 1037 2307 3185 2000 3422 1999 1996 3500 7292 1010 2030 2151 2051 2005 2008 3043 1012 2009 1005 1055 2200 2104 9250 1025 1045 2196 2657 2055 2009 2127 1045 2179 2009 1999 1996 2678 3573 1010 1998 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.567320 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.569100 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.571059 139939410937728 run_classifier.py:468] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.574410 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.576314 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this movie is terrible . it ' s about some no brain surf ##in dude that inherit ##s some company . does carrot top have no shame ? < br / > < br / > [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.578294 139939410937728 run_classifier.py:464] tokens: [CLS] this movie is terrible . it ' s about some no brain surf ##in dude that inherit ##s some company . does carrot top have no shame ? < br / > < br / > [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 3185 2003 6659 1012 2009 1005 1055 2055 2070 2053 4167 14175 2378 12043 2008 22490 2015 2070 2194 1012 2515 25659 2327 2031 2053 9467 1029 1026 7987 1013 1028 1026 7987 1013 1028 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.580309 139939410937728 run_classifier.py:465] input_ids: 101 2023 3185 2003 6659 1012 2009 1005 1055 2055 2070 2053 4167 14175 2378 12043 2008 22490 2015 2070 2194 1012 2515 25659 2327 2031 2053 9467 1029 1026 7987 1013 1028 1026 7987 1013 1028 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.582904 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.584942 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.586766 139939410937728 run_classifier.py:468] label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.601276 139939410937728 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.604540 139939410937728 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i saw this film last night and came online specifically to see if others thought it was as awful as i did . < br / > < br / > granted , obviously some people see a lot in this film that i didn ' t , so if you ' re one of those people , fine - good luck to you . but i ' m a patient person . i ' ve enjoyed extremely long films before . but this was an exercise in torture for me . < br / > < br / > i honestly felt that this was one of those films with little to say , and that it was more about style than substance - however [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.606558 139939410937728 run_classifier.py:464] tokens: [CLS] i saw this film last night and came online specifically to see if others thought it was as awful as i did . < br / > < br / > granted , obviously some people see a lot in this film that i didn ' t , so if you ' re one of those people , fine - good luck to you . but i ' m a patient person . i ' ve enjoyed extremely long films before . but this was an exercise in torture for me . < br / > < br / > i honestly felt that this was one of those films with little to say , and that it was more about style than substance - however [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2387 2023 2143 2197 2305 1998 2234 3784 4919 2000 2156 2065 2500 2245 2009 2001 2004 9643 2004 1045 2106 1012 1026 7987 1013 1028 1026 7987 1013 1028 4379 1010 5525 2070 2111 2156 1037 2843 1999 2023 2143 2008 1045 2134 1005 1056 1010 2061 2065 2017 1005 2128 2028 1997 2216 2111 1010 2986 1011 2204 6735 2000 2017 1012 2021 1045 1005 1049 1037 5776 2711 1012 1045 1005 2310 5632 5186 2146 3152 2077 1012 2021 2023 2001 2019 6912 1999 8639 2005 2033 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 9826 2371 2008 2023 2001 2028 1997 2216 3152 2007 2210 2000 2360 1010 1998 2008 2009 2001 2062 2055 2806 2084 9415 1011 2174 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.608767 139939410937728 run_classifier.py:465] input_ids: 101 1045 2387 2023 2143 2197 2305 1998 2234 3784 4919 2000 2156 2065 2500 2245 2009 2001 2004 9643 2004 1045 2106 1012 1026 7987 1013 1028 1026 7987 1013 1028 4379 1010 5525 2070 2111 2156 1037 2843 1999 2023 2143 2008 1045 2134 1005 1056 1010 2061 2065 2017 1005 2128 2028 1997 2216 2111 1010 2986 1011 2204 6735 2000 2017 1012 2021 1045 1005 1049 1037 5776 2711 1012 1045 1005 2310 5632 5186 2146 3152 2077 1012 2021 2023 2001 2019 6912 1999 8639 2005 2033 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 9826 2371 2008 2023 2001 2028 1997 2216 3152 2007 2210 2000 2360 1010 1998 2008 2009 2001 2062 2055 2806 2084 9415 1011 2174 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.610912 139939410937728 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.613577 139939410937728 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:40:50.615551 139939410937728 run_classifier.py:468] label: 0 (id = 0)\n"],"name":"stderr"}]},{"metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"cell_type":"markdown","source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."]},{"metadata":{"id":"6o2a5ZIvRcJq","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qpE0ZIDOCQzE","colab_type":"text"},"cell_type":"markdown","source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"metadata":{"id":"FnH-AnOQ9KKW","colab_type":"code","colab":{}},"cell_type":"code","source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","colab":{}},"cell_type":"code","source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"emHf9GhfWBZ_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oEJldMr3WYZa","colab_type":"code","colab":{}},"cell_type":"code","source":["# Specify outpit directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=OUTPUT_DIR,\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q_WebpS1X97v","colab_type":"code","outputId":"b25fef9f-38b0-48f5-af22-1cd93923b3bd","executionInfo":{"status":"ok","timestamp":1555983737822,"user_tz":-330,"elapsed":1002,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"cell_type":"code","source":["model_fn = model_fn_builder(\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": BATCH_SIZE})\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f45ecde8400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:42:25.808030 139939410937728 estimator.py:201] Using config: {'_model_dir': 'bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f45ecde8400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"metadata":{"id":"NOO3RfG1DYLo","colab_type":"text"},"cell_type":"markdown","source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."]},{"metadata":{"id":"1Pv2bAlOX_-K","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t6Nukby2EB6-","colab_type":"text"},"cell_type":"markdown","source":["Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."]},{"metadata":{"id":"nucD4gluYJmK","colab_type":"code","outputId":"b84f8dab-305f-4bbc-e835-3bccec2e25fd","executionInfo":{"status":"ok","timestamp":1555983746251,"user_tz":-330,"elapsed":1181,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Skipping training since max_steps has already saved.\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:42:34.132283 139939410937728 estimator.py:351] Skipping training since max_steps has already saved.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:00:00.018116\n"],"name":"stdout"}]},{"metadata":{"id":"CmbLTVniARy3","colab_type":"text"},"cell_type":"markdown","source":["Now let's use our test data to see how well our model did:"]},{"metadata":{"id":"JIhejfpyJ8Bx","colab_type":"code","colab":{}},"cell_type":"code","source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PPVEXhNjYXC-","colab_type":"code","outputId":"880679d5-47c6-4a6a-b0ee-b46bb8a5a6db","executionInfo":{"status":"ok","timestamp":1555986484148,"user_tz":-330,"elapsed":5,"user":{"displayName":"shruti goel","photoUrl":"","userId":"17147634726372402766"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"cell_type":"code","source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:43:57.377673 139939410937728 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:00.750058 139939410937728 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:11.539109 139939410937728 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-04-23T01:44:11Z\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:11.567611 139939410937728 evaluation.py:257] Starting evaluation at 2019-04-23T01:44:11Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:13.221276 139939410937728 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from bert_model/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:13.230787 139939410937728 saver.py:1270] Restoring parameters from bert_model/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:15.337622 139939410937728 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 01:44:15.606696 139939410937728 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-04-23-02:24:10\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 02:24:10.587150 139939410937728 evaluation.py:277] Finished evaluation at 2019-04-23-02:24:10\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.8607431, eval_accuracy = 0.8608, f1_score = 0.8588235, false_negatives = 362.0, false_positives = 334.0, global_step = 468, loss = 0.529705, precision = 0.8637291, recall = 0.8539734, true_negatives = 2187.0, true_positives = 2117.0\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 02:24:10.593217 139939410937728 estimator.py:1979] Saving dict for global step 468: auc = 0.8607431, eval_accuracy = 0.8608, f1_score = 0.8588235, false_negatives = 362.0, false_positives = 334.0, global_step = 468, loss = 0.529705, precision = 0.8637291, recall = 0.8539734, true_negatives = 2187.0, true_positives = 2117.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: bert_model/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["I0423 02:24:13.226771 139939410937728 estimator.py:2039] Saving 'checkpoint_path' summary for global step 468: bert_model/model.ckpt-468\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'auc': 0.8607431,\n"," 'eval_accuracy': 0.8608,\n"," 'f1_score': 0.8588235,\n"," 'false_negatives': 362.0,\n"," 'false_positives': 334.0,\n"," 'global_step': 468,\n"," 'loss': 0.529705,\n"," 'precision': 0.8637291,\n"," 'recall': 0.8539734,\n"," 'true_negatives': 2187.0,\n"," 'true_positives': 2117.0}"]},"metadata":{"tags":[]},"execution_count":51}]},{"metadata":{"id":"ueKsULteiz1B","colab_type":"text"},"cell_type":"markdown","source":["Now let's write code to make predictions on new sentences:"]},{"metadata":{"id":"OsrbTD2EJTVl","colab_type":"code","colab":{}},"cell_type":"code","source":["def getPrediction(in_sentences):\n","  labels = [\"Negative\", \"Positive\"]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n","  predictions = estimator.predict(predict_input_fn)\n","  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-thbodgih_VJ","colab_type":"code","colab":{}},"cell_type":"code","source":["pred_sentences = [\n","  \"That movie was absolutely awful\",\n","  \"The acting was a bit lacking\",\n","  \"The film was creative and surprising\",\n","  \"Absolutely fantastic!\"\n","]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QrZmvZySKQTm","colab_type":"code","outputId":"3891fafb-a460-4eb8-fa6c-335a5bbc10e5","colab":{"base_uri":"https://localhost:8080/","height":649}},"cell_type":"code","source":["predictions = getPrediction(pred_sentences)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n","INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n","INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n","INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n","INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"}]},{"metadata":{"id":"MXkRiEBUqN3n","colab_type":"text"},"cell_type":"markdown","source":["Voila! We have a sentiment classifier!"]},{"metadata":{"id":"ERkTE8-7oQLZ","colab_type":"code","outputId":"26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27","colab":{"base_uri":"https://localhost:8080/","height":221}},"cell_type":"code","source":["predictions"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('That movie was absolutely awful',\n","  array([-4.9142293e-03, -5.3180690e+00], dtype=float32),\n","  'Negative'),\n"," ('The acting was a bit lacking',\n","  array([-0.03325794, -3.4200459 ], dtype=float32),\n","  'Negative'),\n"," ('The film was creative and surprising',\n","  array([-5.3589125e+00, -4.7171740e-03], dtype=float32),\n","  'Positive'),\n"," ('Absolutely fantastic!',\n","  array([-5.0434084 , -0.00647258], dtype=float32),\n","  'Positive')]"]},"metadata":{"tags":[]},"execution_count":73}]}]}