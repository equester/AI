{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled32.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ULFj4OKbPkYt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":507},"outputId":"b8edb7c6-54e8-4a4b-f7e4-c2121d0d5873","executionInfo":{"status":"error","timestamp":1555923138391,"user_tz":-330,"elapsed":8861,"user":{"displayName":"Lavi Nigam","photoUrl":"https://lh6.googleusercontent.com/-VVDbL0W4k54/AAAAAAAAAAI/AAAAAAAAClE/dqqBdY5uP8E/s64/photo.jpg","userId":"01031927567449846531"}}},"cell_type":"code","source":["import tensorflow as tf\n","import glob\n","import imageio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","from tensorflow.keras import layers\n","import time\n","\n","from IPython import display\n","(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n","train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n","train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n","BUFFER_SIZE = 60000\n","BATCH_SIZE = 256\n","# Batch and shuffle the data\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","def make_generator_model():\n","    model = tf.keras.Sequential()\n","    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Reshape((7, 7, 256)))\n","    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n","\n","    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 7, 7, 128)\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 14, 14, 64)\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n","    assert model.output_shape == (None, 28, 28, 1)\n","\n","    return model\n","generator = make_generator_model()\n","\n","def make_discriminator_model():\n","    model = tf.keras.Sequential()\n","    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n","                                     input_shape=[28, 28, 1]))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(1))\n","\n","    return model\n","discriminator = make_discriminator_model()\n","# This method returns a helper function to compute cross entropy loss\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)\n","EPOCHS = 50\n","noise_dim = 100\n","num_examples_to_generate = 16\n","\n","# We will reuse this seed overtime (so it's easier)\n","# to visualize progress in the animated GIF)\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])\n","# Notice the use of `tf.function`\n","# This annotation causes the function to be \"compiled\".\n","@tf.function\n","def train_step(images):\n","    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      generated_images = generator(noise, training=True)\n","\n","      real_output = discriminator(images, training=True)\n","      fake_output = discriminator(generated_images, training=True)\n","\n","      gen_loss = generator_loss(fake_output)\n","      disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","def train(dataset, epochs):\n","  for epoch in range(epochs):\n","    start = time.time()\n","\n","    for image_batch in dataset:\n","      train_step(image_batch)\n","\n","    # Produce images for the GIF as we go\n","    display.clear_output(wait=True)\n","    generate_and_save_images(generator,\n","                             epoch + 1,\n","                             seed)\n","\n","    # Save the model every 15 epochs\n","    if (epoch + 1) % 15 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","\n","  # Generate after the final epoch\n","  display.clear_output(wait=True)\n","  generate_and_save_images(generator,\n","                           epochs,\n","                           seed)\n","def generate_and_save_images(model, epoch, test_input):\n","  # Notice `training` is set to False.\n","  # This is so all layers run in inference mode (batchnorm).\n","  predictions = model(test_input, training=False)\n","\n","  fig = plt.figure(figsize=(4,4))\n","\n","  for i in range(predictions.shape[0]):\n","      plt.subplot(4, 4, i+1)\n","      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n","      plt.axis('off')\n","\n","  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n","  plt.show()\n","train(train_dataset, EPOCHS)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eb462630cf82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                                  \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                  \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                  discriminator=discriminator)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mnoise_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1766\u001b[0m              \u001b[0;34m\"object should be checkpointable (i.e. it is part of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m              \"TensorFlow Python API and manages state), please open an issue.\")\n\u001b[0;32m-> 1768\u001b[0;31m             % (v,))\n\u001b[0m\u001b[1;32m   1769\u001b[0m       \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Created lazily for restore-on-create.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: `Checkpoint` was expecting a checkpointable object (an object derived from `CheckpointableBase`), got <tensorflow.python.keras.optimizers.Adam object at 0x7f2a15082e80>. If you believe this object should be checkpointable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue."]}]}]}